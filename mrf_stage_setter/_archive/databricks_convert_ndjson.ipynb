{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Convert NDJSON to Parquet Files\n",
        "\n",
        "This notebook processes NDJSON files using PySpark, similar to the `convert_to_parquet` command.\n",
        "\n",
        "## Setup\n",
        "\n",
        "1. Upload NDJSON files to Databricks (DBFS or mount point)\n",
        "2. Update the file paths in the configuration cell below\n",
        "3. Run all cells to process files and generate Parquet files (5000 records per file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "# Update these paths to point to your NDJSON files in Databricks\n",
        "\n",
        "# Input: Path to NDJSON files (DBFS path or mounted path)\n",
        "# Example: \"/FileStore/ndjson_files/\" or \"/mnt/databricks/ndjson/\"\n",
        "NDJSON_INPUT_PATH = \"/FileStore/ndjson_files/\"\n",
        "\n",
        "# Output: Path where Parquet files will be saved\n",
        "# Example: \"/FileStore/output_parquet/\" or \"/mnt/databricks/output/\"\n",
        "PARQUET_OUTPUT_PATH = \"/FileStore/output_parquet/\"\n",
        "\n",
        "# Option 1: Process all .ndjson files in the input directory\n",
        "PROCESS_ALL_FILES = True\n",
        "\n",
        "# Option 2: Process specific files (if PROCESS_ALL_FILES is False)\n",
        "SPECIFIC_FILES = [\n",
        "    # \"/FileStore/ndjson_files/file1.ndjson\",\n",
        "    # \"/FileStore/ndjson_files/file2.ndjson\",\n",
        "]\n",
        "\n",
        "# Schema inference: use samplingRatio to control how much of the file is read for schema inference\n",
        "# 1.0 = read entire file (accurate but slow for large files)\n",
        "# 0.1 = read 10% of file (faster but may miss some fields)\n",
        "SCHEMA_SAMPLING_RATIO = 0.1  # Use 0.1 for large files, 1.0 for small files (<100MB)\n",
        "\n",
        "# Number of rows per Parquet file/partition\n",
        "ROWS_PER_PARTITION = 5000\n",
        "\n",
        "print(f\"Configuration:\")\n",
        "print(f\"  Input path: {NDJSON_INPUT_PATH}\")\n",
        "print(f\"  Output path: {PARQUET_OUTPUT_PATH}\")\n",
        "print(f\"  Process all files: {PROCESS_ALL_FILES}\")\n",
        "print(f\"  Schema sampling ratio: {SCHEMA_SAMPLING_RATIO}\")\n",
        "print(f\"  Rows per Parquet partition: {ROWS_PER_PARTITION:,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "from pyspark.sql import SparkSession, functions as F\n",
        "from pyspark.sql.types import StructType, ArrayType\n",
        "from pyspark.sql.window import Window\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# Get or create SparkSession (Databricks already has one)\n",
        "spark = SparkSession.getActiveSession()\n",
        "if spark is None:\n",
        "    spark = SparkSession.builder.appName(\"NDJSONConverter\").getOrCreate()\n",
        "\n",
        "print(\"SparkSession initialized\")\n",
        "print(f\"Spark version: {spark.version}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper function: Auto-flatten DataFrame\n",
        "def auto_flatten(df):\n",
        "    \"\"\"\n",
        "    Recursively explode array<struct> columns and flatten struct columns into top-level columns.\n",
        "    \n",
        "    Column naming: Uses only the leaf key name, appending _1, _2, ... if names collide.\n",
        "    \"\"\"\n",
        "    iteration = 0\n",
        "    while True:\n",
        "        iteration += 1\n",
        "        schema = df.schema\n",
        "        \n",
        "        struct_cols = [\n",
        "            f for f in schema.fields\n",
        "            if isinstance(f.dataType, StructType)\n",
        "        ]\n",
        "        \n",
        "        array_struct_cols = [\n",
        "            f for f in schema.fields\n",
        "            if isinstance(f.dataType, ArrayType)\n",
        "            and isinstance(f.dataType.elementType, StructType)\n",
        "        ]\n",
        "        \n",
        "        if not struct_cols and not array_struct_cols:\n",
        "            # Nothing left to flatten/explode\n",
        "            break\n",
        "        \n",
        "        print(f\"Flatten iteration {iteration}: {len(struct_cols)} struct cols, {len(array_struct_cols)} array<struct> cols\")\n",
        "        \n",
        "        # 1) Explode all array<struct> columns\n",
        "        for f in array_struct_cols:\n",
        "            print(f\"  Exploding array<struct> column: {f.name}\")\n",
        "            df = df.withColumn(f.name, F.explode_outer(F.col(f.name)))\n",
        "        \n",
        "        # 2) Flatten all struct columns\n",
        "        if struct_cols:\n",
        "            struct_names = [f.name for f in struct_cols]\n",
        "            print(f\"  Flattening struct columns: {struct_names}\")\n",
        "            \n",
        "            # Track used names so we don't collide\n",
        "            used_names = set(df.columns)\n",
        "            new_cols = []\n",
        "            \n",
        "            for f in df.schema.fields:\n",
        "                if isinstance(f.dataType, StructType):\n",
        "                    # Replace struct with its fields\n",
        "                    for subf in f.dataType.fields:\n",
        "                        base_name = subf.name  # Only last-level key\n",
        "                        new_name = base_name\n",
        "                        \n",
        "                        # Avoid collisions by appending _1, _2, ...\n",
        "                        i = 1\n",
        "                        while new_name in used_names:\n",
        "                            new_name = f\"{base_name}_{i}\"\n",
        "                            i += 1\n",
        "                        \n",
        "                        used_names.add(new_name)\n",
        "                        new_cols.append(F.col(f\"{f.name}.{subf.name}\").alias(new_name))\n",
        "                else:\n",
        "                    # Keep non-struct column as-is\n",
        "                    new_cols.append(F.col(f.name))\n",
        "            \n",
        "            df = df.select(new_cols)\n",
        "    \n",
        "    print(f\"Flattening complete after {iteration} iteration(s)\")\n",
        "    return df\n",
        "\n",
        "print(\"auto_flatten function defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper function: Restructure negotiated rate DataFrame\n",
        "def restructure_negotiated_rate_df(df):\n",
        "    \"\"\"\n",
        "    Convert mixed negotiated-rate dataset into a unified dataframe.\n",
        "    \n",
        "    Steps:\n",
        "    1. Split billing rows and provider rows\n",
        "    2. Join billing rows to provider rows based on provider_references array\n",
        "    3. Aggregate all NPIs from all matching providers into a single all_npis column\n",
        "    4. Keep one row per billing record (no explosion of provider_references)\n",
        "    \"\"\"\n",
        "    # Check if required columns exist\n",
        "    if \"provider_group_id\" not in df.columns or \"provider_references\" not in df.columns:\n",
        "        print(\"Missing required columns for restructure_negotiated_rate_df. Skipping restructuring.\")\n",
        "        return df\n",
        "    \n",
        "    # 1. Identify billing rows (billing_code not null)\n",
        "    billing_df = df.filter(F.col(\"billing_code\").isNotNull())\n",
        "    \n",
        "    # 2. Identify provider metadata rows (provider_group_id not null)\n",
        "    provider_df = df.filter(F.col(\"provider_group_id\").isNotNull())\n",
        "    \n",
        "    # 3. Select only provider-specific columns from provider_df\n",
        "    billing_cols_set = set(billing_df.columns)\n",
        "    provider_cols_to_keep = [\"provider_group_id\"]\n",
        "    \n",
        "    # Rename npi to provider_npi to avoid ambiguity\n",
        "    if \"npi\" in provider_df.columns:\n",
        "        provider_df = provider_df.withColumnRenamed(\"npi\", \"provider_npi\")\n",
        "        provider_cols_to_keep.append(\"provider_npi\")\n",
        "    \n",
        "    # Add any other columns from provider_df that don't exist in billing_df\n",
        "    for col_name in provider_df.columns:\n",
        "        if col_name not in billing_cols_set and col_name not in provider_cols_to_keep and col_name != \"provider_group_id\":\n",
        "            provider_cols_to_keep.append(col_name)\n",
        "    \n",
        "    # Select only needed columns from provider_df\n",
        "    provider_df_clean = provider_df.select(*provider_cols_to_keep)\n",
        "    \n",
        "    # 4. Temporarily explode provider_references to join, then aggregate back\n",
        "    billing_df_with_id = billing_df.withColumn(\n",
        "        \"_billing_row_id\", \n",
        "        F.monotonically_increasing_id()\n",
        "    )\n",
        "    \n",
        "    # Explode provider_references for the join\n",
        "    billing_exploded = billing_df_with_id.withColumn(\n",
        "        \"provider_reference\", \n",
        "        F.explode_outer(\"provider_references\")\n",
        "    )\n",
        "    \n",
        "    # 5. Join on provider_reference == provider_group_id\n",
        "    provider_df_for_join = provider_df_clean.withColumnRenamed(\"provider_group_id\", \"_join_provider_group_id\")\n",
        "    \n",
        "    joined = billing_exploded.join(\n",
        "        provider_df_for_join,\n",
        "        on=[billing_exploded[\"provider_reference\"] == provider_df_for_join[\"_join_provider_group_id\"]],\n",
        "        how=\"left\"\n",
        "    )\n",
        "    \n",
        "    # Drop temporary columns used for join\n",
        "    joined = joined.drop(\"provider_reference\", \"_join_provider_group_id\")\n",
        "    \n",
        "    # 6. Group back by _billing_row_id and aggregate all NPIs\n",
        "    aggregated = joined.groupBy(\"_billing_row_id\").agg(\n",
        "        # Take first value of each billing column\n",
        "        *[F.first(col).alias(col) for col in billing_df.columns if col != \"provider_references\"],\n",
        "        # Keep provider_references as-is\n",
        "        F.first(\"provider_references\").alias(\"provider_references\"),\n",
        "        # Collect all NPIs from all matched providers into a single flattened array\n",
        "        F.array_distinct(\n",
        "            F.flatten(\n",
        "                F.collect_list(F.coalesce(F.col(\"provider_npi\"), F.array()))\n",
        "            )\n",
        "        ).alias(\"all_npis\")\n",
        "    ).drop(\"_billing_row_id\")\n",
        "    \n",
        "    return aggregated\n",
        "\n",
        "print(\"restructure_negotiated_rate_df function defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper function: Process a single NDJSON file\n",
        "def process_ndjson_file(file_path, output_path, schema_sampling_ratio=0.1, rows_per_partition=5000):\n",
        "    \"\"\"\n",
        "    Process a single NDJSON file:\n",
        "    1. Infer schema (with sampling)\n",
        "    2. Read file with schema\n",
        "    3. Apply auto_flatten\n",
        "    4. Apply restructure_negotiated_rate_df if applicable\n",
        "    5. Write to Parquet files (with specified rows per partition)\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Processing: {file_path}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    try:\n",
        "        # Step 1: Infer schema from file (with sampling)\n",
        "        print(f\"Step 1: Inferring schema (sampling ratio: {schema_sampling_ratio})...\")\n",
        "        df_schema = spark.read.option(\"samplingRatio\", str(schema_sampling_ratio)).json(file_path)\n",
        "        schema = df_schema.schema\n",
        "        print(f\"  Inferred schema with {len(schema.fields)} fields\")\n",
        "        \n",
        "        # Step 2: Read file with inferred schema\n",
        "        print(\"Step 2: Reading NDJSON file with schema...\")\n",
        "        df = spark.read.schema(schema).json(file_path)\n",
        "        initial_row_count = df.count()\n",
        "        print(f\"  Initial row count: {initial_row_count:,}\")\n",
        "        \n",
        "        # Step 3: Apply auto_flatten\n",
        "        print(\"Step 3: Flattening nested structures...\")\n",
        "        df = auto_flatten(df)\n",
        "        flattened_row_count = df.count()\n",
        "        print(f\"  Flattened row count: {flattened_row_count:,}\")\n",
        "        print(f\"  Columns after flattening: {len(df.columns)}\")\n",
        "        \n",
        "        # Step 4: Apply restructure_negotiated_rate_df if applicable\n",
        "        print(\"Step 4: Checking if restructure_negotiated_rate_df is applicable...\")\n",
        "        if \"provider_group_id\" in df.columns and \"provider_references\" in df.columns:\n",
        "            print(\"  Applying restructure_negotiated_rate_df...\")\n",
        "            df = restructure_negotiated_rate_df(df)\n",
        "            restructured_row_count = df.count()\n",
        "            print(f\"  Restructured row count: {restructured_row_count:,}\")\n",
        "        else:\n",
        "            print(\"  Skipping restructure_negotiated_rate_df (required columns not present)\")\n",
        "        \n",
        "        # Step 5: Calculate number of partitions needed\n",
        "        row_count = df.count()\n",
        "        num_partitions = max(1, (row_count + rows_per_partition - 1) // rows_per_partition)\n",
        "        print(f\"\\nStep 5: Writing to Parquet files...\")\n",
        "        print(f\"  Total rows: {row_count:,}\")\n",
        "        print(f\"  Rows per partition: {rows_per_partition:,}\")\n",
        "        print(f\"  Number of partitions: {num_partitions}\")\n",
        "        \n",
        "        # Step 6: Write to Parquet with repartitioning\n",
        "        # Generate output directory name (based on input filename)\n",
        "        file_name = Path(file_path).name.replace('.ndjson', '')\n",
        "        parquet_output_dir = f\"{output_path.rstrip('/')}/{file_name}/\"\n",
        "        \n",
        "        # Repartition to get approximately rows_per_partition rows per partition\n",
        "        # Use repartition (not coalesce) to allow both increasing and decreasing partitions\n",
        "        df_repartitioned = df.repartition(num_partitions)\n",
        "        \n",
        "        # Write to Parquet\n",
        "        df_repartitioned.write.mode(\"overwrite\").parquet(parquet_output_dir)\n",
        "        \n",
        "        print(f\"  Saved to: {parquet_output_dir}\")\n",
        "        print(f\"  Total rows written: {row_count:,}\")\n",
        "        print(f\"  Columns: {len(df.columns)}\")\n",
        "        \n",
        "        # Show a preview of the data\n",
        "        print(\"\\nPreview of data (first 10 rows):\")\n",
        "        display(df.limit(10))\n",
        "        \n",
        "        return {\n",
        "            \"success\": True,\n",
        "            \"file_path\": file_path,\n",
        "            \"output_dir\": parquet_output_dir,\n",
        "            \"total_rows\": row_count,\n",
        "            \"partitions\": num_partitions,\n",
        "            \"rows_per_partition\": rows_per_partition,\n",
        "            \"columns\": len(df.columns)\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"ERROR processing {file_path}: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return {\n",
        "            \"success\": False,\n",
        "            \"file_path\": file_path,\n",
        "            \"error\": str(e)\n",
        "        }\n",
        "\n",
        "print(\"process_ndjson_file function defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get list of files to process\n",
        "import os\n",
        "\n",
        "if PROCESS_ALL_FILES:\n",
        "    # List all .ndjson files in the input directory\n",
        "    print(f\"Listing .ndjson files in: {NDJSON_INPUT_PATH}\")\n",
        "    files = []\n",
        "    try:\n",
        "        # Use dbutils to list files (Databricks utility)\n",
        "        file_list = dbutils.fs.ls(NDJSON_INPUT_PATH)\n",
        "        files = [f.path for f in file_list if f.name.endswith('.ndjson')]\n",
        "        print(f\"Found {len(files)} .ndjson file(s)\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error listing files: {e}\")\n",
        "        print(\"Trying alternative method...\")\n",
        "        # Alternative: use Spark to list files\n",
        "        try:\n",
        "            files_df = spark.read.format(\"binaryFile\").load(f\"{NDJSON_INPUT_PATH}*.ndjson\")\n",
        "            files = [row.path for row in files_df.select(\"path\").distinct().collect()]\n",
        "            print(f\"Found {len(files)} .ndjson file(s) using alternative method\")\n",
        "        except Exception as e2:\n",
        "            print(f\"Alternative method also failed: {e2}\")\n",
        "            files = []\n",
        "else:\n",
        "    # Use specific files\n",
        "    files = SPECIFIC_FILES\n",
        "    print(f\"Processing {len(files)} specific file(s)\")\n",
        "\n",
        "if not files:\n",
        "    print(\"ERROR: No files to process!\")\n",
        "else:\n",
        "    print(\"\\nFiles to process:\")\n",
        "    for i, f in enumerate(files, 1):\n",
        "        print(f\"  {i}. {f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process all files\n",
        "if not files:\n",
        "    print(\"No files to process. Please check the configuration.\")\n",
        "else:\n",
        "    # Create output directory if it doesn't exist\n",
        "    try:\n",
        "        dbutils.fs.mkdirs(PARQUET_OUTPUT_PATH)\n",
        "        print(f\"Output directory ready: {PARQUET_OUTPUT_PATH}\")\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    # Process each file\n",
        "    results = []\n",
        "    for file_path in files:\n",
        "        result = process_ndjson_file(\n",
        "            file_path=file_path,\n",
        "            output_path=PARQUET_OUTPUT_PATH,\n",
        "            schema_sampling_ratio=SCHEMA_SAMPLING_RATIO,\n",
        "            rows_per_partition=ROWS_PER_PARTITION\n",
        "        )\n",
        "        results.append(result)\n",
        "    \n",
        "    # Summary\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"PROCESSING SUMMARY\")\n",
        "    print(f\"{'='*60}\")\n",
        "    successful = [r for r in results if r.get(\"success\")]\n",
        "    failed = [r for r in results if not r.get(\"success\")]\n",
        "    \n",
        "    print(f\"Total files processed: {len(results)}\")\n",
        "    print(f\"  Successful: {len(successful)}\")\n",
        "    print(f\"  Failed: {len(failed)}\")\n",
        "    \n",
        "    if successful:\n",
        "        total_rows = sum(r.get(\"total_rows\", 0) for r in successful)\n",
        "        total_partitions = sum(r.get(\"partitions\", 0) for r in successful)\n",
        "        avg_columns = sum(r.get(\"columns\", 0) for r in successful) / len(successful) if successful else 0\n",
        "        print(f\"\\nSuccessful files:\")\n",
        "        print(f\"  Total rows processed: {total_rows:,}\")\n",
        "        print(f\"  Total Parquet partitions created: {total_partitions}\")\n",
        "        print(f\"  Average columns per file: {avg_columns:.0f}\")\n",
        "        print(f\"\\nOutput directories:\")\n",
        "        for r in successful:\n",
        "            print(f\"  - {r['output_dir']}\")\n",
        "            print(f\"    ({r['total_rows']:,} total rows in {r['partitions']} partition(s), ~{r['rows_per_partition']:,} rows per partition)\")\n",
        "    \n",
        "    if failed:\n",
        "        print(f\"\\nFailed files:\")\n",
        "        for r in failed:\n",
        "            print(f\"  - {r['file_path']}: {r.get('error', 'Unknown error')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notes\n",
        "\n",
        "- **Schema Inference**: For large files, use `SCHEMA_SAMPLING_RATIO = 0.1` (10%) to speed up processing. For small files or if you need 100% accuracy, use `1.0` (read entire file).\n",
        "- **Output Files**: Parquet files are saved to subdirectories (one per input file) in the output path. Each Parquet file contains approximately `ROWS_PER_PARTITION` (5000) rows.\n",
        "- **Partitioning**: The data is repartitioned to create files with approximately the specified number of rows per partition. The actual number of partitions depends on the total row count.\n",
        "- **Databricks Paths**: \n",
        "  - Use `/FileStore/` for files uploaded through Databricks UI\n",
        "  - Use `/mnt/` for mounted cloud storage (S3, ADLS, etc.)\n",
        "  - Use `dbfs:/` prefix for DBFS paths\n",
        "- **Memory**: For very large files, consider processing one file at a time or increasing cluster size.\n",
        "- **Reading Parquet Files**: You can read the output Parquet files using:\n",
        "  ```python\n",
        "  df = spark.read.parquet(\"/FileStore/output_parquet/filename/\")\n",
        "  ```\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
