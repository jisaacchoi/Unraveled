{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Large JSON.gz Files\n",
    "\n",
    "This notebook splits large JSON.gz files by chunking top-level arrays.\n",
    "\n",
    "**How it works:**\n",
    "1. Gets structure info from `mrf_landing` table (or detects from file as fallback)\n",
    "2. Identifies top-level scalars (repeated in every output file)\n",
    "3. Identifies top-level arrays (all arrays chunked by same chunk_size)\n",
    "4. Writes output files - each file gets up to chunk_size items from each array\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Input file structure:\n",
    "{\n",
    "    \"reporting_entity_name\": \"Blue Cross\",  # scalar - repeated in all outputs\n",
    "    \"version\": \"1.0.0\",                      # scalar - repeated in all outputs\n",
    "    \"provider_references\": [...10 items...], # small array - fits in first chunk\n",
    "    \"in_network\": [...500000 items...]       # large array - spans multiple files\n",
    "}\n",
    "\n",
    "With chunk_size=100000, outputs:\n",
    "- input_part001.json.gz: scalars + provider_references[0:10] (all) + in_network[0:100000]\n",
    "- input_part002.json.gz: scalars + in_network[100000:200000]  (provider_references exhausted)\n",
    "- input_part003.json.gz: scalars + in_network[200000:300000]\n",
    "- ...etc\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database connection: localhost:5432/postgres\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "# ============ CONFIGURE THESE ============\n",
    "\n",
    "# Input file path\n",
    "INPUT_FILE = Path(\"D:/payer_mrf/test/2025-12_690_08D0_in-network-rates_12_of_35.json.gz\")\n",
    "\n",
    "# Output directory (where split files will be written)\n",
    "OUTPUT_DIR = Path(\"D:/payer_mrf/test/split\")\n",
    "\n",
    "# Number of items per chunk for the largest array\n",
    "CHUNK_SIZE = 1000\n",
    "\n",
    "# Minimum file size in MB to trigger splitting (set to 0 to disable)\n",
    "MIN_FILE_SIZE_MB = 0\n",
    "\n",
    "# =========================================\n",
    "\n",
    "# Load database connection string from config.yaml\n",
    "CONFIG_PATH = Path(\"../config.yaml\")\n",
    "\n",
    "def get_connection_string_from_config(config_path: Path) -> str:\n",
    "    \"\"\"Build connection string from config.yaml database settings.\"\"\"\n",
    "    with open(config_path, \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    db_config = config.get(\"database\", {})\n",
    "    \n",
    "    # Check if full connection string is provided\n",
    "    conn_str = db_config.get(\"connection_string\", \"\")\n",
    "    if conn_str:\n",
    "        return conn_str\n",
    "    \n",
    "    # Build from individual components\n",
    "    host = db_config.get(\"host\", \"localhost\")\n",
    "    port = db_config.get(\"port\", 5432)\n",
    "    database = db_config.get(\"database\", \"postgres\")\n",
    "    username = db_config.get(\"username\", \"postgres\")\n",
    "    password = db_config.get(\"password\", \"\")\n",
    "    sslmode = db_config.get(\"sslmode\", \"\")\n",
    "    \n",
    "    conn_str = f\"postgresql://{username}:{password}@{host}:{port}/{database}\"\n",
    "    if sslmode:\n",
    "        conn_str += f\"?sslmode={sslmode}\"\n",
    "    \n",
    "    return conn_str\n",
    "\n",
    "DB_CONNECTION_STRING = get_connection_string_from_config(CONFIG_PATH)\n",
    "print(f\"Database connection: {DB_CONNECTION_STRING.split('@')[1] if '@' in DB_CONNECTION_STRING else 'configured'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import logging\n",
    "from decimal import Decimal\n",
    "from typing import Any, Dict, Iterator, List, Tuple, Optional\n",
    "\n",
    "import ijson\n",
    "import psycopg2\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(levelname)s - %(message)s\",\n",
    ")\n",
    "LOG = logging.getLogger(\"split_json_gz\")\n",
    "\n",
    "\n",
    "def convert_decimals(obj: Any) -> Any:\n",
    "    \"\"\"Recursively convert Decimal objects to float for JSON serialization.\"\"\"\n",
    "    if isinstance(obj, Decimal):\n",
    "        return float(obj)\n",
    "    if isinstance(obj, dict):\n",
    "        return {key: convert_decimals(value) for key, value in obj.items()}\n",
    "    if isinstance(obj, list):\n",
    "        return [convert_decimals(item) for item in obj]\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_json_file(file_path: Path):\n",
    "    \"\"\"Open a JSON file (compressed or uncompressed).\"\"\"\n",
    "    if file_path.suffix == \".gz\" or str(file_path).endswith(\".json.gz\"):\n",
    "        return gzip.open(file_path, \"rt\", encoding=\"utf-8\")\n",
    "    else:\n",
    "        return open(file_path, \"r\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_structure_from_mrf_landing(\n",
    "    connection_string: str,\n",
    "    file_name: str,\n",
    ") -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Query mrf_landing to get structure info for a file.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (scalar_keys, array_keys) where:\n",
    "        - scalar_keys: list of record_types that are scalars (max record_index = 0)\n",
    "        - array_keys: list of record_types that are arrays (max record_index > 0)\n",
    "    \"\"\"\n",
    "    LOG.info(f\"Querying mrf_landing for structure of '{file_name}'...\")\n",
    "    \n",
    "    conn = psycopg2.connect(connection_string)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    try:\n",
    "        # Get record_type and max record_index for each type\n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT record_type, MAX(record_index) as max_index\n",
    "            FROM mrf_landing\n",
    "            WHERE file_name = %s\n",
    "            GROUP BY record_type\n",
    "            ORDER BY record_type\n",
    "        \"\"\", (file_name,))\n",
    "        \n",
    "        rows = cursor.fetchall()\n",
    "        \n",
    "        scalar_keys = []\n",
    "        array_keys = []\n",
    "        \n",
    "        for record_type, max_index in rows:\n",
    "            if max_index == 0:\n",
    "                # Scalar (or array with 1 item, treated as scalar)\n",
    "                scalar_keys.append(record_type)\n",
    "            else:\n",
    "                # Array with multiple items\n",
    "                array_keys.append(record_type)\n",
    "        \n",
    "        LOG.info(f\"Found {len(scalar_keys)} scalar(s): {scalar_keys}\")\n",
    "        LOG.info(f\"Found {len(array_keys)} array(s): {array_keys}\")\n",
    "        \n",
    "        return scalar_keys, array_keys\n",
    "        \n",
    "    finally:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "\n",
    "def detect_structure(file_path: Path) -> Tuple[Dict[str, Any], List[str]]:\n",
    "    \"\"\"\n",
    "    Detect top-level structure by parsing the file: identify scalars/objects vs arrays.\n",
    "    (Fallback when mrf_landing data is not available)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (scalars_dict, array_keys) where:\n",
    "        - scalars_dict: dict of scalar/object values to repeat in every output\n",
    "        - array_keys: list of keys that are arrays (to be chunked)\n",
    "    \"\"\"\n",
    "    scalars = {}\n",
    "    array_keys = []\n",
    "    \n",
    "    LOG.info(\"Detecting top-level structure from file...\")\n",
    "    \n",
    "    with open_json_file(file_path) as fh:\n",
    "        parser = ijson.parse(fh)\n",
    "        current_key = None\n",
    "        depth = 0\n",
    "        \n",
    "        for prefix, event, value in parser:\n",
    "            if event == \"start_map\":\n",
    "                depth += 1\n",
    "            elif event == \"end_map\":\n",
    "                depth -= 1\n",
    "                if depth == 0:\n",
    "                    break\n",
    "            elif event == \"start_array\" and depth == 1 and current_key:\n",
    "                array_keys.append(current_key)\n",
    "                current_key = None\n",
    "            elif event == \"map_key\" and depth == 1:\n",
    "                current_key = value\n",
    "            elif event in (\"string\", \"number\", \"boolean\", \"null\") and depth == 1 and current_key:\n",
    "                scalars[current_key] = value\n",
    "                current_key = None\n",
    "    \n",
    "    LOG.info(f\"Found {len(scalars)} scalar(s): {list(scalars.keys())}\")\n",
    "    LOG.info(f\"Found {len(array_keys)} array(s): {array_keys}\")\n",
    "    \n",
    "    return scalars, array_keys\n",
    "\n",
    "\n",
    "def extract_scalars(file_path: Path, scalar_keys: List[str]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extract scalar values from file for the given keys.\n",
    "    \"\"\"\n",
    "    scalars = {}\n",
    "    \n",
    "    LOG.info(f\"Extracting {len(scalar_keys)} scalar values from file...\")\n",
    "    \n",
    "    with open_json_file(file_path) as fh:\n",
    "        parser = ijson.parse(fh)\n",
    "        current_key = None\n",
    "        depth = 0\n",
    "        \n",
    "        for prefix, event, value in parser:\n",
    "            if event == \"start_map\":\n",
    "                depth += 1\n",
    "            elif event == \"end_map\":\n",
    "                depth -= 1\n",
    "                if depth == 0:\n",
    "                    break\n",
    "            elif event == \"map_key\" and depth == 1:\n",
    "                current_key = value\n",
    "            elif event in (\"string\", \"number\", \"boolean\", \"null\") and depth == 1 and current_key:\n",
    "                if current_key in scalar_keys:\n",
    "                    scalars[current_key] = value\n",
    "                current_key = None\n",
    "    \n",
    "    return scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_all_arrays(file_path: Path, array_keys: List[str], chunk_size: int) -> Iterator[Dict[str, List[Any]]]:\n",
    "    \"\"\"\n",
    "    Stream all arrays from file, yielding chunks of up to chunk_size items per array.\n",
    "    \n",
    "    Each yield is a dict of {array_key: [items...]} for that chunk.\n",
    "    Arrays that run out of items simply won't appear in subsequent chunks.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to JSON file\n",
    "        array_keys: List of array keys to stream\n",
    "        chunk_size: Max items per array per chunk\n",
    "        \n",
    "    Yields:\n",
    "        Dict of {array_key: [items]} for each chunk\n",
    "    \"\"\"\n",
    "    # Create iterators for each array\n",
    "    iterators = {}\n",
    "    file_handles = []\n",
    "    \n",
    "    for key in array_keys:\n",
    "        fh = open_json_file(file_path)\n",
    "        file_handles.append(fh)\n",
    "        ijson_path = f\"{key}.item\"\n",
    "        iterators[key] = ijson.items(fh, ijson_path)\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            chunk = {}\n",
    "            any_items = False\n",
    "            \n",
    "            # Collect up to chunk_size items from each array\n",
    "            for key, iterator in iterators.items():\n",
    "                items = []\n",
    "                for _ in range(chunk_size):\n",
    "                    try:\n",
    "                        item = next(iterator)\n",
    "                        items.append(item)\n",
    "                        any_items = True\n",
    "                    except StopIteration:\n",
    "                        break\n",
    "                \n",
    "                if items:\n",
    "                    chunk[key] = items\n",
    "            \n",
    "            if not any_items:\n",
    "                break\n",
    "                \n",
    "            yield chunk\n",
    "            \n",
    "    finally:\n",
    "        for fh in file_handles:\n",
    "            fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (stream_array_items removed - using stream_all_arrays instead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (extract_full_array removed - using stream_all_arrays instead)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Split Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_json_gz(\n",
    "    input_path: Path,\n",
    "    output_dir: Path,\n",
    "    chunk_size: int = 100000,\n",
    "    min_file_size_mb: float = 0,\n",
    "    connection_string: Optional[str] = None,\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Split a JSON.gz file by chunking all top-level arrays.\n",
    "    \n",
    "    All arrays are chunked by the same chunk_size. When an array runs out of items,\n",
    "    it simply doesn't appear in subsequent output files.\n",
    "    \n",
    "    Args:\n",
    "        input_path: Path to input JSON.gz file\n",
    "        output_dir: Directory to write output files\n",
    "        chunk_size: Number of items per chunk for each array\n",
    "        min_file_size_mb: Minimum file size (MB) to trigger splitting (0 to disable)\n",
    "        connection_string: Database connection string to query mrf_landing (optional)\n",
    "        \n",
    "    Returns:\n",
    "        Number of output files created\n",
    "    \"\"\"\n",
    "    if not input_path.exists():\n",
    "        LOG.error(f\"Input file not found: {input_path}\")\n",
    "        return 0\n",
    "    \n",
    "    # Check file size\n",
    "    file_size_mb = input_path.stat().st_size / (1024 * 1024)\n",
    "    LOG.info(f\"Input file: {input_path.name} ({file_size_mb:.1f} MB)\")\n",
    "    \n",
    "    if min_file_size_mb > 0 and file_size_mb < min_file_size_mb:\n",
    "        LOG.info(f\"File size ({file_size_mb:.1f} MB) is below threshold ({min_file_size_mb} MB), skipping split\")\n",
    "        return 0\n",
    "    \n",
    "    file_name = input_path.name\n",
    "    \n",
    "    # Try to get structure from mrf_landing first\n",
    "    scalar_keys = None\n",
    "    array_keys = None\n",
    "    \n",
    "    if connection_string:\n",
    "        try:\n",
    "            scalar_keys, array_keys = get_structure_from_mrf_landing(connection_string, file_name)\n",
    "        except Exception as e:\n",
    "            LOG.warning(f\"Could not query mrf_landing: {e}. Falling back to file parsing.\")\n",
    "    \n",
    "    # If we got structure from mrf_landing, extract scalar values from file\n",
    "    if scalar_keys is not None and array_keys is not None:\n",
    "        scalars = extract_scalars(input_path, scalar_keys)\n",
    "    else:\n",
    "        # Fallback: detect structure from file\n",
    "        scalars, array_keys = detect_structure(input_path)\n",
    "    \n",
    "    if not array_keys:\n",
    "        LOG.warning(\"No arrays found in file, nothing to split\")\n",
    "        return 0\n",
    "    \n",
    "    LOG.info(f\"Will chunk {len(array_keys)} array(s): {array_keys}\")\n",
    "    LOG.info(f\"Chunk size: {chunk_size:,} items per array per file\")\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Generate output filename pattern\n",
    "    stem = input_path.name\n",
    "    if stem.endswith(\".json.gz\"):\n",
    "        stem = stem[:-8]\n",
    "    elif stem.endswith(\".json\"):\n",
    "        stem = stem[:-5]\n",
    "    \n",
    "    # Stream all arrays and write chunks\n",
    "    LOG.info(f\"Streaming arrays and writing chunks...\")\n",
    "    \n",
    "    files_created = 0\n",
    "    \n",
    "    for chunk_data in stream_all_arrays(input_path, array_keys, chunk_size):\n",
    "        files_created += 1\n",
    "        output_path = output_dir / f\"{stem}_part{files_created:03d}.json.gz\"\n",
    "        \n",
    "        # Build output JSON: scalars + array chunks\n",
    "        output_data = dict(scalars)  # Start with scalars (always included)\n",
    "        output_data.update(chunk_data)  # Add array chunks\n",
    "        \n",
    "        # Convert Decimals to floats for JSON serialization\n",
    "        output_data = convert_decimals(output_data)\n",
    "        \n",
    "        # Log what's in this chunk\n",
    "        chunk_info = \", \".join([f\"{k}: {len(v):,}\" for k, v in chunk_data.items()])\n",
    "        LOG.info(f\"Writing {output_path.name} ({chunk_info})...\")\n",
    "        \n",
    "        with gzip.open(output_path, \"wt\", encoding=\"utf-8\") as f:\n",
    "            json.dump(output_data, f)\n",
    "    \n",
    "    LOG.info(f\"Split complete: created {files_created} files in {output_dir}\")\n",
    "    return files_created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-21 22:34:03,551 INFO - Input file: 2025-12_690_08D0_in-network-rates_12_of_35.json.gz (14.4 MB)\n",
      "2026-01-21 22:34:03,552 INFO - Querying mrf_landing for structure of '2025-12_690_08D0_in-network-rates_12_of_35.json.gz'...\n",
      "2026-01-21 22:34:04,418 INFO - Found 4 scalar(s): ['last_updated_on', 'reporting_entity_name', 'reporting_entity_type', 'version']\n",
      "2026-01-21 22:34:04,419 INFO - Found 2 array(s): ['in_network', 'provider_references']\n",
      "2026-01-21 22:34:04,420 INFO - Extracting 4 scalar values from file...\n",
      "2026-01-21 22:35:51,214 INFO - Will chunk 2 array(s): ['in_network', 'provider_references']\n",
      "2026-01-21 22:35:51,232 INFO - Chunk size: 1,000 items per array per file\n",
      "2026-01-21 22:35:51,237 INFO - Streaming arrays and writing chunks...\n"
     ]
    }
   ],
   "source": [
    "# Run the split\n",
    "files_created = split_json_gz(\n",
    "    input_path=INPUT_FILE,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    min_file_size_mb=MIN_FILE_SIZE_MB,\n",
    "    connection_string=DB_CONNECTION_STRING,  # Uses mrf_landing for structure info\n",
    ")\n",
    "\n",
    "print(f\"\\nFiles created: {files_created}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Process Multiple Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and modify to process multiple files in a directory\n",
    "\n",
    "# INPUT_DIR = Path(\"D:/payer_mrf/raw/test/2025-11-25\")\n",
    "# OUTPUT_DIR = Path(\"D:/payer_mrf/raw/test/2025-11-25/split\")\n",
    "# \n",
    "# json_gz_files = list(INPUT_DIR.glob(\"*.json.gz\"))\n",
    "# print(f\"Found {len(json_gz_files)} JSON.gz files\")\n",
    "# \n",
    "# total_files_created = 0\n",
    "# for input_file in json_gz_files:\n",
    "#     print(f\"\\nProcessing: {input_file.name}\")\n",
    "#     files_created = split_json_gz(\n",
    "#         input_path=input_file,\n",
    "#         output_dir=OUTPUT_DIR,\n",
    "#         chunk_size=CHUNK_SIZE,\n",
    "#         min_file_size_mb=MIN_FILE_SIZE_MB,\n",
    "#     )\n",
    "#     total_files_created += files_created\n",
    "# \n",
    "# print(f\"\\nTotal files created: {total_files_created}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
