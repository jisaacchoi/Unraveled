{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "550d6ac9",
   "metadata": {},
   "source": [
    "# Minimal Gzip + JSON Indexer (Overlap-Safe, Unique Top-Level Keys)\n",
    "\n",
    "This notebook indexes **top-level keys** in large `.json.gz` files without a full JSON parse.\n",
    "\n",
    "## What was fixed\n",
    "- **No array-as-scalar bugs**: value classification happens only after the first non-whitespace token after `:`.\n",
    "- **Unique top-level keys persisted across chunks**: each top-level key is processed/logged once across the whole stream.\n",
    "- **Overlap-safe state retention**: if a token spans chunks (e.g., an incomplete key or string scalar), the notebook preserves only the unfinished fragment; otherwise it keeps a small fixed overlap tail.\n",
    "- **Progress reporting** based on compressed bytes read.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c5512e",
   "metadata": {},
   "source": [
    "## Install dependency\n",
    "\n",
    "```bash\n",
    "pip install indexed_gzip\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "265873a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional, Set, Tuple\n",
    "\n",
    "import indexed_gzip as igzip\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Logging\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\"\n",
    ")\n",
    "LOG = logging.getLogger(__name__)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Data Structures\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "@dataclass\n",
    "class JsonIndex:\n",
    "    \"\"\"Minimal structural index for a top-level JSON object.\"\"\"\n",
    "    top_level_offsets: Dict[str, int] = field(default_factory=dict)\n",
    "    scalar_values: Dict[str, str] = field(default_factory=dict)\n",
    "\n",
    "    # Persisted across all chunks: each top-level key should be observed once.\n",
    "    seen_top_level_keys: Set[str] = field(default_factory=set)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Helpers\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def is_whitespace(ch: str) -> bool:\n",
    "    return ch in \" \\t\\n\\r\"\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Core Indexing Logic\n",
    "#   - Defers value classification until first non-whitespace token\n",
    "#   - Logs progress based on compressed bytes\n",
    "#   - Persists unique top-level keys (only log/process once across all chunks)\n",
    "#   - Overlap-safe: if a token is incomplete at a chunk boundary, keep exactly\n",
    "#     the unfinished fragment; otherwise keep a fixed overlap tail.\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def build_index(\n",
    "    path_gz: Path,\n",
    "    json_index_file: Path,\n",
    "    gzip_index_file: Optional[Path] = None,\n",
    "    spacing: int = 300 * 1024,\n",
    "    chunk_size: int = 256 * 1024,\n",
    "    overlap_size: int = 4096,\n",
    "    progress_every_mb: int = 256,\n",
    "    log_new_keys: bool = True,\n",
    ") -> JsonIndex:\n",
    "    \"\"\"Build a minimal index of top-level JSON keys from a .json.gz file.\n",
    "\n",
    "    Captures:\n",
    "      - Offsets for top-level arrays (position where '[' begins in decompressed stream)\n",
    "      - Scalar values for top-level scalars (strings, numbers, true/false/null)\n",
    "\n",
    "    Key guarantees:\n",
    "      - Each top-level key is processed once across all chunks (tracked in seen_top_level_keys).\n",
    "      - Overlap handling retains state only when needed: if a token spans chunks,\n",
    "        the unfinished fragment is preserved; otherwise we keep the last overlap_size chars.\n",
    "\n",
    "    Progress:\n",
    "      - Based on compressed bytes read from the .gz file (fh.fileobj.tell()).\n",
    "    \"\"\"\n",
    "\n",
    "    gz_size = path_gz.stat().st_size\n",
    "    report_interval = max(1, progress_every_mb) * 1024 * 1024\n",
    "    next_report = report_interval\n",
    "\n",
    "    LOG.info(\n",
    "        \"Indexing %s (compressed size %.2f GB)\",\n",
    "        path_gz,\n",
    "        gz_size / (1024 ** 3),\n",
    "    )\n",
    "\n",
    "    index = JsonIndex()\n",
    "\n",
    "    with igzip.IndexedGzipFile(\n",
    "        filename=str(path_gz),\n",
    "        index_file=str(gzip_index_file) if gzip_index_file else None,\n",
    "        spacing=spacing,\n",
    "    ) as fh:\n",
    "\n",
    "        buf = \"\"\n",
    "        # pos is the decompressed character position corresponding to buf[0]\n",
    "        pos = 0\n",
    "\n",
    "        depth = 0\n",
    "        in_string = False\n",
    "        escape = False\n",
    "\n",
    "        current_key: Optional[str] = None\n",
    "        awaiting_value = False\n",
    "\n",
    "        while True:\n",
    "            chunk = fh.read(chunk_size)\n",
    "            if not chunk:\n",
    "                break\n",
    "\n",
    "            # -----------------------------\n",
    "            # Progress reporting (compressed bytes)\n",
    "            # -----------------------------\n",
    "            try:\n",
    "                comp_pos = fh.fileobj.tell()\n",
    "                if comp_pos >= next_report:\n",
    "                    pct = (comp_pos / gz_size) * 100 if gz_size else 0.0\n",
    "                    LOG.info(\n",
    "                        \"Progress: %.1f%% (%.2f / %.2f GB compressed) | unique_top_keys=%d\",\n",
    "                        pct,\n",
    "                        comp_pos / (1024 ** 3),\n",
    "                        gz_size / (1024 ** 3),\n",
    "                        len(index.seen_top_level_keys),\n",
    "                    )\n",
    "                    next_report += report_interval\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            text = chunk.decode(\"utf-8\", errors=\"ignore\")\n",
    "            buf += text\n",
    "\n",
    "            i = 0\n",
    "            keep_from: Optional[int] = None  # if set, preserve buf[keep_from:] (unfinished token)\n",
    "\n",
    "            while i < len(buf):\n",
    "                ch = buf[i]\n",
    "\n",
    "                # ---------------------------------------------------------\n",
    "                # Awaiting value classification (only for a NEW top-level key)\n",
    "                # ---------------------------------------------------------\n",
    "                if awaiting_value and depth == 1 and current_key is not None:\n",
    "                    if is_whitespace(ch):\n",
    "                        i += 1\n",
    "                        continue\n",
    "\n",
    "                    # Array value -> record offset at '['\n",
    "                    if ch == \"[\":\n",
    "                        index.top_level_offsets[current_key] = pos + i\n",
    "                        current_key = None\n",
    "                        awaiting_value = False\n",
    "                        i += 1\n",
    "                        continue\n",
    "\n",
    "                    # Object value at top level -> not a scalar; ignore but maintain depth\n",
    "                    if ch == \"{\":\n",
    "                        current_key = None\n",
    "                        awaiting_value = False\n",
    "                        depth += 1\n",
    "                        i += 1\n",
    "                        continue\n",
    "\n",
    "                    # Scalar value\n",
    "                    start = i\n",
    "\n",
    "                    if ch == '\"':\n",
    "                        # Need to parse a quoted string scalar safely across chunks\n",
    "                        in_string = True\n",
    "                        escape = False\n",
    "                        i += 1\n",
    "                        while i < len(buf):\n",
    "                            c2 = buf[i]\n",
    "                            if escape:\n",
    "                                escape = False\n",
    "                            elif c2 == \"\\\\\":\n",
    "                                escape = True\n",
    "                            elif c2 == '\"':\n",
    "                                in_string = False\n",
    "                                i += 1\n",
    "                                break\n",
    "                            i += 1\n",
    "\n",
    "                        if in_string:\n",
    "                            # string scalar spans chunks: keep from its starting quote\n",
    "                            keep_from = start\n",
    "                            break\n",
    "\n",
    "                        # After string, consume whitespace; do NOT eat delimiters\n",
    "                        while i < len(buf) and is_whitespace(buf[i]):\n",
    "                            i += 1\n",
    "\n",
    "                        value = buf[start:i].strip()\n",
    "                        index.scalar_values[current_key] = value\n",
    "                        current_key = None\n",
    "                        awaiting_value = False\n",
    "                        continue\n",
    "\n",
    "                    else:\n",
    "                        # Non-string scalar: read until ',' or '}' (top-level object end)\n",
    "                        while i < len(buf) and buf[i] not in \",}\":\n",
    "                            i += 1\n",
    "                        value = buf[start:i].strip()\n",
    "                        index.scalar_values[current_key] = value\n",
    "                        current_key = None\n",
    "                        awaiting_value = False\n",
    "                        continue\n",
    "\n",
    "                # ---------------------------------------------------------\n",
    "                # General string handling (outside of the scalar-string path)\n",
    "                # ---------------------------------------------------------\n",
    "                if in_string:\n",
    "                    if escape:\n",
    "                        escape = False\n",
    "                    elif ch == \"\\\\\":\n",
    "                        escape = True\n",
    "                    elif ch == '\"':\n",
    "                        in_string = False\n",
    "                    i += 1\n",
    "                    continue\n",
    "\n",
    "                # ---------------------------------------------------------\n",
    "                # Key detection (top-level keys only): \"key\" :\n",
    "                # ---------------------------------------------------------\n",
    "                if ch == '\"':\n",
    "                    key_quote_start = i  # for boundary preservation if needed\n",
    "                    start = i + 1\n",
    "                    end = buf.find('\"', start)\n",
    "                    if end == -1:\n",
    "                        # key name spans chunks: keep from starting quote\n",
    "                        keep_from = key_quote_start\n",
    "                        break\n",
    "\n",
    "                    key = buf[start:end]\n",
    "                    i = end + 1\n",
    "\n",
    "                    # Skip whitespace\n",
    "                    while i < len(buf) and is_whitespace(buf[i]):\n",
    "                        i += 1\n",
    "\n",
    "                    # Confirm ':' and top-level depth\n",
    "                    if i < len(buf) and buf[i] == \":\" and depth == 1:\n",
    "                        # Persist only once across all chunks\n",
    "                        if key not in index.seen_top_level_keys:\n",
    "                            index.seen_top_level_keys.add(key)\n",
    "                            if log_new_keys:\n",
    "                                LOG.info(\"Discovered top-level key: %s\", key)\n",
    "\n",
    "                            current_key = key\n",
    "                            awaiting_value = True\n",
    "                        else:\n",
    "                            # Key already handled; do not enter awaiting_value\n",
    "                            current_key = None\n",
    "                            awaiting_value = False\n",
    "\n",
    "                        i += 1  # consume ':'\n",
    "                    continue\n",
    "\n",
    "                # ---------------------------------------------------------\n",
    "                # Structural depth tracking\n",
    "                # ---------------------------------------------------------\n",
    "                if ch == \"{\":\n",
    "                    depth += 1\n",
    "                elif ch == \"}\":\n",
    "                    depth -= 1\n",
    "\n",
    "                i += 1\n",
    "\n",
    "            # ---------------------------------------------------------\n",
    "            # Overlap retention\n",
    "            #   - If token unfinished, keep exactly from keep_from onward.\n",
    "            #   - Otherwise keep a fixed tail overlap.\n",
    "            # ---------------------------------------------------------\n",
    "            if keep_from is not None:\n",
    "                # Advance pos by the amount we are discarding\n",
    "                pos += keep_from\n",
    "                buf = buf[keep_from:]\n",
    "            else:\n",
    "                if len(buf) > overlap_size:\n",
    "                    discard = len(buf) - overlap_size\n",
    "                    pos += discard\n",
    "                    buf = buf[-overlap_size:]\n",
    "\n",
    "    # Persist index (include seen keys count for auditing)\n",
    "    json_index_file.write_text(\n",
    "        json.dumps(\n",
    "            {\n",
    "                \"offsets\": index.top_level_offsets,\n",
    "                \"scalars\": index.scalar_values,\n",
    "                \"seen_top_level_keys_count\": len(index.seen_top_level_keys),\n",
    "                \"seen_top_level_keys\": sorted(index.seen_top_level_keys),\n",
    "            },\n",
    "            indent=2,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    LOG.info(\"Index written to %s\", json_index_file)\n",
    "    return index\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Convenience Wrapper (avoid FileNotFoundError if .gzidx doesn't exist)\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def build_index_wrapper(input_file: Path, output_dir: Optional[Path] = None) -> JsonIndex:\n",
    "    output_dir = output_dir or input_file.parent\n",
    "\n",
    "    json_index_file = output_dir / f\"{input_file.name}.index.json\"\n",
    "    gzip_index_file = output_dir / f\"{input_file.name}.gzidx\"\n",
    "\n",
    "    # Only pass index_file if it already exists (some versions error otherwise).\n",
    "    gzip_index_arg = gzip_index_file if gzip_index_file.exists() else None\n",
    "\n",
    "    return build_index(\n",
    "        path_gz=input_file,\n",
    "        json_index_file=json_index_file,\n",
    "        gzip_index_file=gzip_index_arg,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680907c4",
   "metadata": {},
   "source": [
    "## Example usage\n",
    "\n",
    "Update the path below to point to your `.json.gz` file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76aa2bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-26 15:05:37,890 | INFO | Indexing D:\\_ingested_2026-01_720_27B0_in-network-rates_01_of_57.json.gz (compressed size 0.02 GB)\n",
      "2026-01-26 15:05:37,898 | INFO | Discovered top-level key: reporting_entity_name\n",
      "2026-01-26 15:05:37,899 | INFO | Discovered top-level key: reporting_entity_type\n",
      "2026-01-26 15:05:37,899 | INFO | Discovered top-level key: last_updated_on\n",
      "2026-01-26 15:05:37,900 | INFO | Discovered top-level key: version\n",
      "2026-01-26 15:05:37,901 | INFO | Discovered top-level key: provider_references\n",
      "2026-01-26 15:05:37,928 | INFO | Discovered top-level key: in_network\n",
      "2026-01-26 15:05:43,416 | INFO | Discovered top-level key: negotiation_arrangement\n",
      "2026-01-26 15:05:43,417 | INFO | Discovered top-level key: name\n",
      "2026-01-26 15:05:43,418 | INFO | Discovered top-level key: billing_code_type\n",
      "2026-01-26 15:05:43,418 | INFO | Discovered top-level key: billing_code_type_version\n",
      "2026-01-26 15:05:43,419 | INFO | Discovered top-level key: billing_code\n",
      "2026-01-26 15:05:43,420 | INFO | Discovered top-level key: description\n",
      "2026-01-26 15:05:43,421 | INFO | Discovered top-level key: negotiated_rates\n",
      "2026-01-26 15:05:43,588 | INFO | Discovered top-level key: negotiated_prices\n",
      "2026-01-26 15:05:43,772 | INFO | Discovered top-level key: billing_class\n",
      "2026-01-26 15:05:43,773 | INFO | Discovered top-level key: billing_code_modifier\n",
      "2026-01-26 15:05:43,773 | INFO | Discovered top-level key: negotiated_type\n",
      "2026-01-26 15:05:43,774 | INFO | Discovered top-level key: negotiated_rate\n",
      "2026-01-26 15:05:43,775 | INFO | Discovered top-level key: expiration_date\n",
      "2026-01-26 15:05:43,776 | INFO | Discovered top-level key: service_code\n",
      "2026-01-26 15:06:25,257 | INFO | Index written to D:\\_ingested_2026-01_720_27B0_in-network-rates_01_of_57.json.gz.index.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'provider_references': 195, 'in_network': 539926, 'negotiated_rates': 39167079, 'negotiated_prices': 40104521, 'billing_code_modifier': 41414778, 'service_code': 41415061}\n",
      "{'reporting_entity_name': '\"Blue Cross and Blue Shield of Minnesota\"', 'reporting_entity_type': '\"Health insurance Issuer\"', 'last_updated_on': '\"2025-11-23\"', 'version': '\"1.3.1\"', 'negotiation_arrangement': '\"ffs\"', 'name': '\"OUTPATIENT CARE\"', 'billing_code_type': '\"CPT\"', 'billing_code_type_version': '\"2025\"', 'billing_code': '\"99283\"', 'description': '\"EMERGENCY DEPARTMENT VISIT FOR THE EVALUATION AND MANAGEMENT OF A PATIENT, WHICH REQUIRES A MEDICALLY APPROPRIATE HISTORY AND/OR EXAMINATION AND LOW LEVEL OF MEDICAL DECISION MAKING \"', 'billing_class': '\"professional\"', 'negotiated_type': '\"negotiated\"', 'negotiated_rate': '73.77', 'expiration_date': '\"9999-12-31\"'}\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Example:\n",
    "input_file = Path(r\"D:\\\\_ingested_2026-01_720_27B0_in-network-rates_01_of_57.json.gz\")\n",
    "idx = build_index_wrapper(input_file)\n",
    "print(idx.top_level_offsets)\n",
    "print(idx.scalar_values)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
