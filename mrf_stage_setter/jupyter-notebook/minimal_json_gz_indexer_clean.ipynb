{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6e94bd3",
   "metadata": {},
   "source": [
    "# Minimal Gzip + JSON Indexer for Massive MRF Files\n",
    "\n",
    "This notebook provides a **minimal structural indexer** for very large `.json.gz` files (e.g., MRF).\n",
    "\n",
    "## What it does\n",
    "- Streams a `.json.gz` using `indexed_gzip` (supports random access via a gzip index)\n",
    "- Scans for **top-level JSON keys**\n",
    "- Records offsets for **top-level array values** (e.g., `\"in_network\"`, `\"provider_references\"`) without full parsing\n",
    "- Captures **top-level scalar values** (e.g., `\"reporting_entity_name\"`) when present\n",
    "\n",
    "## Outputs\n",
    "- `<file>.index.json`: JSON index with offsets + scalars\n",
    "- `<file>.gzidx`: gzip seek index created/used by `indexed_gzip`\n",
    "\n",
    "## Notes\n",
    "This is **not** a full JSON parser by design. It is intended for the common MRF pattern:\n",
    "- top-level object\n",
    "- large arrays at top-level keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d55e0387",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional\n",
    "\n",
    "import indexed_gzip as igzip\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Logging\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\"\n",
    ")\n",
    "LOG = logging.getLogger(__name__)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Data Structures\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "@dataclass\n",
    "class JsonIndex:\n",
    "    \"\"\"Minimal structural index for a top-level JSON object.\"\"\"\n",
    "    top_level_offsets: Dict[str, int] = field(default_factory=dict)\n",
    "    scalar_values: Dict[str, str] = field(default_factory=dict)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Helpers\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def is_whitespace(ch: str) -> bool:\n",
    "    return ch in \" \\t\\n\\r\"\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Core Indexing Logic\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def build_index(\n",
    "    path_gz: Path,\n",
    "    json_index_file: Path,\n",
    "    gzip_index_file: Optional[Path] = None,\n",
    "    spacing: int = 300 * 1024,\n",
    "    chunk_size: int = 10000 * 1024,\n",
    "    overlap_size: int = 10*1024,\n",
    "    progress_every_mb: int = 5,\n",
    ") -> JsonIndex:\n",
    "    \"\"\"\n",
    "    Build a minimal index of top-level JSON keys from a .json.gz file.\n",
    "\n",
    "    Progress reporting is based on COMPRESSED bytes read from the .gz file,\n",
    "    which is a reliable proxy for elapsed work.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path_gz : Path\n",
    "        Input .json.gz file\n",
    "    json_index_file : Path\n",
    "        Output index JSON path\n",
    "    gzip_index_file : Optional[Path]\n",
    "        Optional gzip index path for indexed_gzip\n",
    "    spacing : int\n",
    "        Gzip index spacing (bytes)\n",
    "    chunk_size : int\n",
    "        Read size (bytes, decompressed)\n",
    "    overlap_size : int\n",
    "        Text overlap retained between chunks\n",
    "    progress_every_mb : int\n",
    "        Emit progress log every N MB of compressed input processed\n",
    "    \"\"\"\n",
    "\n",
    "    gz_size = path_gz.stat().st_size\n",
    "    report_interval = progress_every_mb\n",
    "    next_report = report_interval\n",
    "\n",
    "    LOG.info(\n",
    "        \"Indexing %s (compressed size %.2f GB)\",\n",
    "        path_gz,\n",
    "        gz_size / (1024 ** 3),\n",
    "    )\n",
    "\n",
    "    index = JsonIndex()\n",
    "\n",
    "    with igzip.IndexedGzipFile(\n",
    "        filename=str(path_gz),\n",
    "        index_file=str(gzip_index_file) if gzip_index_file else None,\n",
    "        spacing=spacing,\n",
    "    ) as fh:\n",
    "\n",
    "        buf = \"\"\n",
    "        pos = 0  # approx decompressed character position\n",
    "\n",
    "        depth = 0\n",
    "        in_string = False\n",
    "        escape = False\n",
    "        current_key = None\n",
    "\n",
    "        while True:\n",
    "            chunk = fh.read(chunk_size)\n",
    "            if not chunk:\n",
    "                break\n",
    "\n",
    "            # ---------------------------------------------------------\n",
    "            # Progress reporting (compressed bytes)\n",
    "            # ---------------------------------------------------------\n",
    "            try:\n",
    "                comp_pos = fh.fileobj.tell()\n",
    "                if comp_pos >= next_report:\n",
    "                    pct = (comp_pos / gz_size) * 100 if gz_size else 0.0\n",
    "                    LOG.info(\n",
    "                        \"Progress: %.1f%% (%.2f / %.2f GB compressed)\",\n",
    "                        pct,\n",
    "                        comp_pos / (1024 ** 3),\n",
    "                        gz_size / (1024 ** 3),\n",
    "                    )\n",
    "                    next_report += report_interval\n",
    "            except Exception:\n",
    "                # Do not fail indexing if progress reporting is unavailable\n",
    "                pass\n",
    "\n",
    "            text = chunk.decode(\"utf-8\", errors=\"ignore\")\n",
    "            buf += text\n",
    "\n",
    "            i = 0\n",
    "            while i < len(buf):\n",
    "                ch = buf[i]\n",
    "\n",
    "                # -----------------------------\n",
    "                # String handling\n",
    "                # -----------------------------\n",
    "                if in_string:\n",
    "                    if escape:\n",
    "                        escape = False\n",
    "                    elif ch == \"\\\\\":\n",
    "                        escape = True\n",
    "                    elif ch == '\"':\n",
    "                        in_string = False\n",
    "                    i += 1\n",
    "                    continue\n",
    "\n",
    "                if ch == '\"':\n",
    "                    start = i + 1\n",
    "                    end = buf.find('\"', start)\n",
    "                    if end == -1:\n",
    "                        break  # wait for next chunk\n",
    "\n",
    "                    key = buf[start:end]\n",
    "                    i = end + 1\n",
    "\n",
    "                    # Skip whitespace\n",
    "                    while i < len(buf) and is_whitespace(buf[i]):\n",
    "                        i += 1\n",
    "\n",
    "                    # Top-level key\n",
    "                    if i < len(buf) and buf[i] == \":\" and depth == 1:\n",
    "                        current_key = key\n",
    "                    continue\n",
    "\n",
    "                # -----------------------------\n",
    "                # Structural characters\n",
    "                # -----------------------------\n",
    "                if ch == \"{\":\n",
    "                    depth += 1\n",
    "                elif ch == \"}\":\n",
    "                    depth -= 1\n",
    "                elif ch == \"[\":\n",
    "                    # Top-level array start\n",
    "                    if depth == 1 and current_key:\n",
    "                        index.top_level_offsets[current_key] = pos + i\n",
    "                        current_key = None\n",
    "                elif ch not in \",:\" and depth == 1 and current_key:\n",
    "                    # Scalar value\n",
    "                    start = i\n",
    "                    while i < len(buf) and buf[i] not in \",}\":\n",
    "                        i += 1\n",
    "                    value = buf[start:i].strip()\n",
    "                    index.scalar_values[current_key] = value\n",
    "                    current_key = None\n",
    "                    continue\n",
    "\n",
    "                i += 1\n",
    "\n",
    "            # Retain overlap for boundary safety\n",
    "            pos += len(text)\n",
    "            buf = buf[-overlap_size:]\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Persist index\n",
    "    # ---------------------------------------------------------\n",
    "    json_index_file.write_text(\n",
    "        json.dumps(\n",
    "            {\n",
    "                \"offsets\": index.top_level_offsets,\n",
    "                \"scalars\": index.scalar_values,\n",
    "            },\n",
    "            indent=2,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    LOG.info(\"Index written to %s\", json_index_file)\n",
    "    return index\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Convenience Wrapper\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def build_index_wrapper(input_file: Path, output_dir: Optional[Path] = None) -> JsonIndex:\n",
    "    output_dir = output_dir or input_file.parent\n",
    "\n",
    "    json_index_file = output_dir / f\"{input_file.name}.index.json\"\n",
    "    gzip_index_file = output_dir / f\"{input_file.name}.gzidx\"\n",
    "\n",
    "    # Only pass an index_file if it already exists (avoids FileNotFoundError)\n",
    "    gzip_index_arg = gzip_index_file if gzip_index_file.exists() else None\n",
    "\n",
    "    return build_index(\n",
    "        path_gz=input_file,\n",
    "        json_index_file=json_index_file,\n",
    "        gzip_index_file=gzip_index_arg,\n",
    "    )\n",
    "    index_file=str(gzip_index_file) if gzip_index_file else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2634cf",
   "metadata": {},
   "source": [
    "## Example usage\n",
    "\n",
    "Update the path below to point to your `.json.gz` MRF file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d4d9d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-26 14:55:04,438 | INFO | Indexing D:\\_ingested_2026-01_720_27B0_in-network-rates_01_of_57.json.gz (compressed size 0.02 GB)\n",
      "2026-01-26 14:56:00,507 | INFO | Index written to D:\\_ingested_2026-01_720_27B0_in-network-rates_01_of_57.json.gz.index.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'negotiated_rates': 9107278},\n",
       " {'reporting_entity_name': '\"Blue Cross and Blue Shield of Minnesota\"',\n",
       "  'reporting_entity_type': '\"Health insurance Issuer\"',\n",
       "  'last_updated_on': '\"2025-11-23\"',\n",
       "  'version': '\"1.3.1\"',\n",
       "  'provider_references': '[{\"provider_group_id\":720.0000237894',\n",
       "  'location': '\"https://mrfdata.hmhs.com/files/720/mn/inbound/local/providergrp/bcbsa/720_pdo_prov_mrf_prvgrp_11_0000265241.json\"',\n",
       "  'provider_group_id': '720.0000265241'})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example:\n",
    "input_file = Path(\"D://_ingested_2026-01_720_27B0_in-network-rates_01_of_57.json.gz\")\n",
    "idx = build_index_wrapper(input_file)\n",
    "idx.top_level_offsets, idx.scalar_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1094f3a1",
   "metadata": {},
   "source": [
    "## Interpreting output\n",
    "\n",
    "- `offsets`: position (in the decompressed text stream) where the top-level array begins (`[`)\n",
    "- `scalars`: stringified scalar values captured at the top-level\n",
    "\n",
    "If you want offset-based partial extraction helpers (e.g., read the first N items of `in_network`), tell me and I will add them as additional cells."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
