{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimal Gzip + JSON Indexer for Partial Extraction\n",
    "\n",
    "This notebook implements a system that indexes massive .json.gz files (e.g., 10GB+) to enable partial extraction of JSON content without full decompression or full parsing.\n",
    "\n",
    "## Features\n",
    "- Gzip seek checkpoints for random access\n",
    "- Minimal JSON structural index (array start or object key value starts)\n",
    "- Single-pass indexing algorithm\n",
    "- Partial extraction without full decompression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "from dataclasses import dataclass, asdict\n",
    "from io import BytesIO\n",
    "import struct\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "LOG = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GzipCheckpoint:\n",
    "    \"\"\"A checkpoint for gzip decompression state.\"\"\"\n",
    "    uncompressed_offset: int\n",
    "    compressed_offset: int\n",
    "    decompressor_state: bytes  # Pickled gzip decompressor state\n",
    "    \n",
    "    def to_dict(self) -> dict:\n",
    "        return {\n",
    "            'uncompressed_offset': self.uncompressed_offset,\n",
    "            'compressed_offset': self.compressed_offset,\n",
    "            'decompressor_state': self.decompressor_state.hex()  # Store as hex for JSON serialization\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dict(cls, d: dict) -> 'GzipCheckpoint':\n",
    "        return cls(\n",
    "            uncompressed_offset=d['uncompressed_offset'],\n",
    "            compressed_offset=d['compressed_offset'],\n",
    "            decompressor_state=bytes.fromhex(d['decompressor_state'])\n",
    "        )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class JsonIndex:\n",
    "    \"\"\"Minimal JSON structural index.\"\"\"\n",
    "    is_array: bool\n",
    "    array_start_offset: Optional[int] = None  # For top-level arrays\n",
    "    top_level_keys: Dict[str, int] = None  # For top-level objects: key -> value start offset\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.top_level_keys is None:\n",
    "            self.top_level_keys = {}\n",
    "    \n",
    "    def to_dict(self) -> dict:\n",
    "        return {\n",
    "            'is_array': self.is_array,\n",
    "            'array_start_offset': self.array_start_offset,\n",
    "            'top_level_keys': self.top_level_keys\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dict(cls, d: dict) -> 'JsonIndex':\n",
    "        return cls(\n",
    "            is_array=d['is_array'],\n",
    "            array_start_offset=d.get('array_start_offset'),\n",
    "            top_level_keys=d.get('top_level_keys', {})\n",
    "        )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CombinedIndex:\n",
    "    \"\"\"Combined gzip checkpoints and JSON structural index.\"\"\"\n",
    "    checkpoints: List[GzipCheckpoint]\n",
    "    json_index: JsonIndex\n",
    "    \n",
    "    def to_dict(self) -> dict:\n",
    "        return {\n",
    "            'checkpoints': [cp.to_dict() for cp in self.checkpoints],\n",
    "            'json_index': self.json_index.to_dict()\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dict(cls, d: dict) -> 'CombinedIndex':\n",
    "        return cls(\n",
    "            checkpoints=[GzipCheckpoint.from_dict(cp) for cp in d['checkpoints']],\n",
    "            json_index=JsonIndex.from_dict(d['json_index'])\n",
    "        )\n",
    "    \n",
    "    def save(self, path: Path) -> None:\n",
    "        \"\"\"Save index to JSON file.\"\"\"\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(self.to_dict(), f, indent=2)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path: Path) -> 'CombinedIndex':\n",
    "        \"\"\"Load index from JSON file.\"\"\"\n",
    "        with open(path, 'r') as f:\n",
    "            return cls.from_dict(json.load(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gzip Decompressor State Management\n",
    "\n",
    "Note: Python's gzip module doesn't expose decompressor state directly. We'll use a workaround by creating a custom decompressor wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GzipDecompressorState:\n",
    "    \"\"\"Wrapper to capture and restore gzip decompressor state.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.decompressor = gzip.GzipFile(fileobj=BytesIO(), mode='rb')\n",
    "        self.buffer = BytesIO()\n",
    "        self.total_uncompressed = 0\n",
    "    \n",
    "    def decompress(self, data: bytes) -> bytes:\n",
    "        \"\"\"Decompress data chunk.\"\"\"\n",
    "        # For simplicity, we'll use a streaming approach\n",
    "        # In production, you might want to use zlib directly for better state control\n",
    "        result = self.decompressor.decompress(data)\n",
    "        self.total_uncompressed += len(result)\n",
    "        return result\n",
    "    \n",
    "    def snapshot(self) -> bytes:\n",
    "        \"\"\"Create a snapshot of decompressor state.\"\"\"\n",
    "        # Note: This is a simplified approach. In production, you'd need to\n",
    "        # serialize the actual zlib decompressor state, which requires\n",
    "        # using zlib.decompressobj() and capturing its internal state.\n",
    "        # For now, we'll store position info that can help with seeking.\n",
    "        state = {\n",
    "            'total_uncompressed': self.total_uncompressed,\n",
    "            # In a real implementation, you'd serialize zlib decompressor state\n",
    "        }\n",
    "        return pickle.dumps(state)\n",
    "    \n",
    "    @classmethod\n",
    "    def restore(cls, state_bytes: bytes) -> 'GzipDecompressorState':\n",
    "        \"\"\"Restore decompressor from snapshot.\"\"\"\n",
    "        state = pickle.loads(state_bytes)\n",
    "        obj = cls()\n",
    "        obj.total_uncompressed = state['total_uncompressed']\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single-Pass Combined Indexing Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_whitespace(ch: str) -> bool:\n",
    "    \"\"\"Check if character is whitespace.\"\"\"\n",
    "    return ch in ' \\t\\n\\r'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_combined_index(\n",
    "    path_gz: Path,\n",
    "    checkpoint_span_uncompressed: int = 64 * 1024  # 64KB default\n",
    ") -> CombinedIndex:\n",
    "    \"\"\"\n",
    "    Build combined gzip checkpoint and JSON structural index in a single pass.\n",
    "    \n",
    "    Args:\n",
    "        path_gz: Path to .json.gz file\n",
    "        checkpoint_span_uncompressed: Create checkpoint every N bytes of uncompressed data\n",
    "    \n",
    "    Returns:\n",
    "        CombinedIndex with checkpoints and JSON structure\n",
    "    \"\"\"\n",
    "    LOG.info(f\"Building index for {path_gz.name}...\")\n",
    "    \n",
    "    # State variables\n",
    "    uncompressed_offset = 0\n",
    "    compressed_offset = 0\n",
    "    next_checkpoint_boundary = checkpoint_span_uncompressed\n",
    "    \n",
    "    # JSON scanning state\n",
    "    in_string = False\n",
    "    escape = False\n",
    "    depth_object = 0\n",
    "    depth_array = 0\n",
    "    \n",
    "    array_start_offset = None\n",
    "    top_level_keys = {}\n",
    "    reading_key = False\n",
    "    current_key_buffer = \"\"\n",
    "    last_key = None\n",
    "    seen_colon_after_key = False\n",
    "    \n",
    "    # Determine if top-level is array or object\n",
    "    is_array = None\n",
    "    \n",
    "    checkpoints = []\n",
    "    decompressor = GzipDecompressorState()\n",
    "    \n",
    "    chunk_size = 8192  # Read 8KB compressed chunks\n",
    "    \n",
    "    with open(path_gz, 'rb') as compressed_file:\n",
    "        while True:\n",
    "            compressed_chunk = compressed_file.read(chunk_size)\n",
    "            if not compressed_chunk:\n",
    "                break\n",
    "            \n",
    "            chunk_compressed_start = compressed_offset\n",
    "            \n",
    "            try:\n",
    "                decompressed_chunk = decompressor.decompress(compressed_chunk)\n",
    "            except Exception as e:\n",
    "                LOG.warning(f\"Decompression error at offset {compressed_offset}: {e}\")\n",
    "                break\n",
    "            \n",
    "            if not decompressed_chunk:\n",
    "                compressed_offset += len(compressed_chunk)\n",
    "                continue\n",
    "            \n",
    "            # Process each byte in decompressed chunk\n",
    "            for byte_val in decompressed_chunk:\n",
    "                ch = chr(byte_val)\n",
    "                \n",
    "                # --- JSON STRUCTURE SCANNING ---\n",
    "                if in_string:\n",
    "                    if escape:\n",
    "                        escape = False\n",
    "                    elif ch == '\\\\':\n",
    "                        escape = True\n",
    "                    elif ch == '\"':\n",
    "                        in_string = False\n",
    "                        # Finish reading a key\n",
    "                        if reading_key:\n",
    "                            last_key = current_key_buffer\n",
    "                            current_key_buffer = \"\"\n",
    "                            reading_key = False\n",
    "                    else:\n",
    "                        if reading_key:\n",
    "                            current_key_buffer += ch\n",
    "                else:\n",
    "                    if ch == '\"':\n",
    "                        in_string = True\n",
    "                        if depth_object == 1 and depth_array == 0:\n",
    "                            reading_key = True\n",
    "                            current_key_buffer = \"\"\n",
    "                    \n",
    "                    elif ch == '{':\n",
    "                        depth_object += 1\n",
    "                        if depth_object == 1 and is_array is None:\n",
    "                            is_array = False\n",
    "                    \n",
    "                    elif ch == '}':\n",
    "                        depth_object -= 1\n",
    "                        if seen_colon_after_key:\n",
    "                            seen_colon_after_key = False\n",
    "                            last_key = None\n",
    "                    \n",
    "                    elif ch == '[':\n",
    "                        depth_array += 1\n",
    "                        if depth_array == 1 and is_array is None:\n",
    "                            is_array = True\n",
    "                    \n",
    "                    elif ch == ']':\n",
    "                        depth_array -= 1\n",
    "                    \n",
    "                    elif ch == ':':\n",
    "                        if last_key is not None and depth_object == 1 and depth_array == 0:\n",
    "                            seen_colon_after_key = True\n",
    "                    \n",
    "                    else:\n",
    "                        # Detect start of array contents (first non-whitespace after '[')\n",
    "                        if (depth_array == 1 and \n",
    "                            array_start_offset is None and \n",
    "                            not is_whitespace(ch) and \n",
    "                            ch != '['):\n",
    "                            array_start_offset = uncompressed_offset\n",
    "                        \n",
    "                        # Detect start of object value (first non-whitespace after ':')\n",
    "                        if (seen_colon_after_key and \n",
    "                            not is_whitespace(ch) and \n",
    "                            last_key is not None and \n",
    "                            last_key not in top_level_keys):\n",
    "                            top_level_keys[last_key] = uncompressed_offset\n",
    "                            seen_colon_after_key = False\n",
    "                            last_key = None\n",
    "                \n",
    "                # --- END JSON SCANNING ---\n",
    "                \n",
    "                uncompressed_offset += 1\n",
    "                \n",
    "                # --- CHECKPOINT CREATION ---\n",
    "                if uncompressed_offset >= next_checkpoint_boundary:\n",
    "                    checkpoint_state = decompressor.snapshot()\n",
    "                    checkpoints.append(GzipCheckpoint(\n",
    "                        uncompressed_offset=uncompressed_offset,\n",
    "                        compressed_offset=chunk_compressed_start,\n",
    "                        decompressor_state=checkpoint_state\n",
    "                    ))\n",
    "                    next_checkpoint_boundary += checkpoint_span_uncompressed\n",
    "                    LOG.debug(f\"Created checkpoint at uncompressed offset {uncompressed_offset}\")\n",
    "            \n",
    "            compressed_offset += len(compressed_chunk)\n",
    "    \n",
    "    # Create JSON index\n",
    "    json_index = JsonIndex(\n",
    "        is_array=is_array if is_array is not None else False,\n",
    "        array_start_offset=array_start_offset,\n",
    "        top_level_keys=top_level_keys\n",
    "    )\n",
    "    \n",
    "    LOG.info(f\"Indexing complete: {len(checkpoints)} checkpoints, \"\n",
    "             f\"JSON type={'array' if json_index.is_array else 'object'}, \"\n",
    "             f\"keys={len(json_index.top_level_keys)}\")\n",
    "    \n",
    "    return CombinedIndex(checkpoints=checkpoints, json_index=json_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial Extraction Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_checkpoint_before(\n",
    "    checkpoints: List[GzipCheckpoint],\n",
    "    target_offset: int\n",
    ") -> Optional[GzipCheckpoint]:\n",
    "    \"\"\"Find the checkpoint with the highest uncompressed_offset <= target_offset.\"\"\"\n",
    "    best_cp = None\n",
    "    for cp in checkpoints:\n",
    "        if cp.uncompressed_offset <= target_offset:\n",
    "            if best_cp is None or cp.uncompressed_offset > best_cp.uncompressed_offset:\n",
    "                best_cp = cp\n",
    "    return best_cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_partial(\n",
    "    path_gz: Path,\n",
    "    checkpoints: List[GzipCheckpoint],\n",
    "    start_offset: int,\n",
    "    max_bytes: int\n",
    ") -> bytes:\n",
    "    \"\"\"\n",
    "    Extract partial content from .json.gz file starting at given uncompressed offset.\n",
    "    \n",
    "    Args:\n",
    "        path_gz: Path to .json.gz file\n",
    "        checkpoints: List of gzip checkpoints\n",
    "        start_offset: Uncompressed byte offset to start extraction\n",
    "        max_bytes: Maximum number of uncompressed bytes to extract\n",
    "    \n",
    "    Returns:\n",
    "        Extracted bytes (may contain incomplete JSON at the end)\n",
    "    \"\"\"\n",
    "    # Find nearest checkpoint before start_offset\n",
    "    cp = find_checkpoint_before(checkpoints, start_offset)\n",
    "    \n",
    "    if cp is None:\n",
    "        # No checkpoint found, start from beginning\n",
    "        LOG.warning(\"No checkpoint found before start_offset, starting from beginning\")\n",
    "        decompressor = GzipDecompressorState()\n",
    "        current_uncompressed = 0\n",
    "        with open(path_gz, 'rb') as f:\n",
    "            compressed_data = f.read()\n",
    "    else:\n",
    "        # Restore from checkpoint\n",
    "        LOG.info(f\"Using checkpoint at uncompressed offset {cp.uncompressed_offset}\")\n",
    "        decompressor = GzipDecompressorState.restore(cp.decompressor_state)\n",
    "        current_uncompressed = cp.uncompressed_offset\n",
    "        with open(path_gz, 'rb') as f:\n",
    "            f.seek(cp.compressed_offset)\n",
    "            compressed_data = f.read()\n",
    "    \n",
    "    # Decompress forward until we reach start_offset, then collect bytes\n",
    "    output = bytearray()\n",
    "    \n",
    "    # Decompress remaining data\n",
    "    try:\n",
    "        decompressed = decompressor.decompress(compressed_data)\n",
    "    except Exception as e:\n",
    "        LOG.error(f\"Decompression error: {e}\")\n",
    "        return bytes(output)\n",
    "    \n",
    "    # Find the slice we need\n",
    "    chunk_start = current_uncompressed\n",
    "    chunk_end = current_uncompressed + len(decompressed)\n",
    "    \n",
    "    if chunk_end <= start_offset:\n",
    "        # Haven't reached start_offset yet\n",
    "        LOG.warning(f\"Decompressed data ends at {chunk_end}, but start_offset is {start_offset}\")\n",
    "        return bytes(output)\n",
    "    \n",
    "    # Calculate slice\n",
    "    slice_from = max(0, start_offset - chunk_start)\n",
    "    slice_to = min(len(decompressed), start_offset + max_bytes - chunk_start)\n",
    "    \n",
    "    if slice_from < len(decompressed):\n",
    "        output.extend(decompressed[slice_from:slice_to])\n",
    "    \n",
    "    LOG.info(f\"Extracted {len(output)} bytes starting at offset {start_offset}\")\n",
    "    return bytes(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Build index for a large .json.gz file\n",
    "def example_build_index(input_file: Path, index_file: Path, checkpoint_span: int = 64 * 1024):\n",
    "    \"\"\"Build and save index.\"\"\"\n",
    "    index = build_combined_index(input_file, checkpoint_span_uncompressed=checkpoint_span)\n",
    "    index.save(index_file)\n",
    "    print(f\"Index saved to {index_file}\")\n",
    "    print(f\"  Checkpoints: {len(index.checkpoints)}\")\n",
    "    print(f\"  JSON type: {'array' if index.json_index.is_array else 'object'}\")\n",
    "    if index.json_index.is_array:\n",
    "        print(f\"  Array start offset: {index.json_index.array_start_offset}\")\n",
    "    else:\n",
    "        print(f\"  Top-level keys: {list(index.json_index.top_level_keys.keys())}\")\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Extract first part of top-level array\n",
    "def example_extract_array_start(\n",
    "    input_file: Path,\n",
    "    index_file: Path,\n",
    "    max_bytes: int = 5 * 1024 * 1024  # 5MB\n",
    ") -> bytes:\n",
    "    \"\"\"Extract first part of top-level array.\"\"\"\n",
    "    index = CombinedIndex.load(index_file)\n",
    "    \n",
    "    if not index.json_index.is_array:\n",
    "        raise ValueError(\"JSON is not a top-level array\")\n",
    "    \n",
    "    if index.json_index.array_start_offset is None:\n",
    "        raise ValueError(\"Array start offset not found in index\")\n",
    "    \n",
    "    start = index.json_index.array_start_offset\n",
    "    bytes_data = extract_partial(input_file, index.checkpoints, start, max_bytes)\n",
    "    \n",
    "    print(f\"Extracted {len(bytes_data)} bytes from array start (offset {start})\")\n",
    "    return bytes_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Extract first part of a top-level key value\n",
    "def example_extract_key_value(\n",
    "    input_file: Path,\n",
    "    index_file: Path,\n",
    "    key_name: str,\n",
    "    max_bytes: int = 5 * 1024 * 1024  # 5MB\n",
    ") -> bytes:\n",
    "    \"\"\"Extract first part of a top-level key's value.\"\"\"\n",
    "    index = CombinedIndex.load(index_file)\n",
    "    \n",
    "    if index.json_index.is_array:\n",
    "        raise ValueError(\"JSON is a top-level array, not an object\")\n",
    "    \n",
    "    if key_name not in index.json_index.top_level_keys:\n",
    "        available_keys = list(index.json_index.top_level_keys.keys())\n",
    "        raise ValueError(f\"Key '{key_name}' not found. Available keys: {available_keys}\")\n",
    "    \n",
    "    start = index.json_index.top_level_keys[key_name]\n",
    "    bytes_data = extract_partial(input_file, index.checkpoints, start, max_bytes)\n",
    "    \n",
    "    print(f\"Extracted {len(bytes_data)} bytes from key '{key_name}' (offset {start})\")\n",
    "    return bytes_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with Sample File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test .json.gz file\n",
    "def create_test_file(output_path: Path, is_array: bool = True, size_mb: float = 0.1):\n",
    "    \"\"\"Create a test .json.gz file for testing.\"\"\"\n",
    "    import tempfile\n",
    "    \n",
    "    # Create temporary JSON file\n",
    "    with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as tmp:\n",
    "        if is_array:\n",
    "            # Create array with many items\n",
    "            tmp.write('[')\n",
    "            target_size = size_mb * 1024 * 1024\n",
    "            written = 1  # '['\n",
    "            item_count = 0\n",
    "            while written < target_size:\n",
    "                if item_count > 0:\n",
    "                    tmp.write(',')\n",
    "                    written += 1\n",
    "                item = json.dumps({\"id\": item_count, \"data\": \"x\" * 100})\n",
    "                tmp.write(item)\n",
    "                written += len(item)\n",
    "                item_count += 1\n",
    "            tmp.write(']')\n",
    "        else:\n",
    "            # Create object with many keys\n",
    "            tmp.write('{')\n",
    "            keys = ['users', 'products', 'orders', 'items', 'data']\n",
    "            for i, key in enumerate(keys):\n",
    "                if i > 0:\n",
    "                    tmp.write(',')\n",
    "                value = json.dumps([{\"id\": j} for j in range(1000)])\n",
    "                tmp.write(f'\"{key}\":{value}')\n",
    "            tmp.write('}')\n",
    "        tmp_path = Path(tmp.name)\n",
    "    \n",
    "    # Compress to .json.gz\n",
    "    with open(tmp_path, 'rb') as f_in:\n",
    "        with gzip.open(output_path, 'wb') as f_out:\n",
    "            f_out.write(f_in.read())\n",
    "    \n",
    "    # Clean up temp file\n",
    "    tmp_path.unlink()\n",
    "    \n",
    "    print(f\"Created test file: {output_path} ({output_path.stat().st_size / 1024:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-26 10:58:14,642 - INFO - Building index for test_large.json.gz...\n",
      "2026-01-26 10:58:14,645 - WARNING - Decompression error at offset 0: 'GzipFile' object has no attribute 'decompress'\n",
      "2026-01-26 10:58:14,646 - INFO - Indexing complete: 0 checkpoints, JSON type=object, keys=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created test file: test_large.json.gz (22.7 KB)\n",
      "Index saved to test_large.json.gz.index\n",
      "  Checkpoints: 0\n",
      "  JSON type: object\n",
      "  Top-level keys: []\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "JSON is not a top-level array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m index \u001b[38;5;241m=\u001b[39m example_build_index(test_file, test_index, checkpoint_span\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m1024\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Extract first 1MB of array\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m array_data \u001b[38;5;241m=\u001b[39m \u001b[43mexample_extract_array_start\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFirst 200 chars: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray_data[:\u001b[38;5;241m200\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36mexample_extract_array_start\u001b[1;34m(input_file, index_file, max_bytes)\u001b[0m\n\u001b[0;32m      8\u001b[0m index \u001b[38;5;241m=\u001b[39m CombinedIndex\u001b[38;5;241m.\u001b[39mload(index_file)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m index\u001b[38;5;241m.\u001b[39mjson_index\u001b[38;5;241m.\u001b[39mis_array:\n\u001b[1;32m---> 11\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJSON is not a top-level array\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index\u001b[38;5;241m.\u001b[39mjson_index\u001b[38;5;241m.\u001b[39marray_start_offset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArray start offset not found in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: JSON is not a top-level array"
     ]
    }
   ],
   "source": [
    "test_file = Path(\"test_large.json.gz\")\n",
    "test_index = Path(\"test_large.json.gz.index\")\n",
    "\n",
    "# Create test file (array)\n",
    "create_test_file(test_file, is_array=True, size_mb=1.0)\n",
    "\n",
    "# Build index\n",
    "index = example_build_index(test_file, test_index, checkpoint_span=64*1024)\n",
    "\n",
    "# Extract first 1MB of array\n",
    "array_data = example_extract_array_start(test_file, test_index, max_bytes=1024*1024)\n",
    "print(f\"First 200 chars: {array_data[:200]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production Notes\n",
    "\n",
    "### Limitations of Current Implementation\n",
    "\n",
    "1. **Gzip State Serialization**: The current implementation uses a simplified approach for gzip state. For production use, you would need to:\n",
    "   - Use `zlib.decompressobj()` directly instead of `gzip.GzipFile()`\n",
    "   - Serialize the actual zlib decompressor state (which is complex)\n",
    "   - Or use a library that supports gzip seeking (like `indexed_gzip`)\n",
    "\n",
    "2. **Checkpoint Accuracy**: The current checkpoints are approximate. For exact seeking, you'd need to:\n",
    "   - Store the exact zlib window state\n",
    "   - Store the bit buffer state\n",
    "   - Handle gzip headers and footers correctly\n",
    "\n",
    "3. **Memory Usage**: For very large files, consider:\n",
    "   - Streaming checkpoint creation\n",
    "   - Incremental index updates\n",
    "   - Using memory-mapped files\n",
    "\n",
    "### Alternative Approaches\n",
    "\n",
    "For production, consider:\n",
    "- Using `indexed_gzip` library (https://github.com/pauldmccarthy/indexed_gzip)\n",
    "- Using `zstandard` compression with seekable format\n",
    "- Pre-processing files into chunked formats (e.g., line-delimited JSON per chunk)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
