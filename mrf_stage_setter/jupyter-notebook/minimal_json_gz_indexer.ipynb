{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimal Gzip + JSON Indexer for MRF Files\n",
    "\n",
    "This notebook implements a system that indexes massive MRF .json.gz files (e.g., 10GB+) to enable partial extraction of top-level JSON keys without full decompression or full parsing.\n",
    "\n",
    "## Features\n",
    "- Uses `indexed_gzip` for fast random access to gzip files\n",
    "- Minimal JSON structural index built with overlapping chunks\n",
    "- Identifies and extracts complete scalar values during indexing\n",
    "- Records start offsets for arrays and objects\n",
    "- **Read-only operations**: Indexing does NOT modify the original .json.gz files\n",
    "\n",
    "## MRF File Structure\n",
    "MRF files are always top-level objects with keys like:\n",
    "- `reporting_entity_name` (scalar)\n",
    "- `provider_references` (array)\n",
    "- `in_network` (array)\n",
    "- `reporting_structure` (object)\n",
    "- etc.\n",
    "\n",
    "The indexer records where each top-level key's value starts, allowing extraction without reading the entire file.\n",
    "\n",
    "## Index Files\n",
    "- **JSON Structural Index** (`.index`): Maps top-level keys to their uncompressed byte offsets\n",
    "- **Gzip Index** (`.gzidx`): Enables fast seeking within the gzip file (managed by `indexed_gzip`)\n",
    "\n",
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import indexed_gzip as igzip\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional\n",
    "from dataclasses import dataclass\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "LOG = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class JsonIndex:\n",
    "    \"\"\"Minimal JSON structural index for top-level objects.\"\"\"\n",
    "    top_level_keys: Dict[str, int] = None  # key -> value start offset (for arrays and objects)\n",
    "    scalar_values: Dict[str, str] = None  # key -> complete scalar string value\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.top_level_keys is None:\n",
    "            self.top_level_keys = {}\n",
    "        if self.scalar_values is None:\n",
    "            self.scalar_values = {}\n",
    "    \n",
    "    def to_dict(self) -> dict:\n",
    "        return {\n",
    "            'top_level_keys': self.top_level_keys,\n",
    "            'scalar_values': self.scalar_values\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dict(cls, d: dict) -> 'JsonIndex':\n",
    "        return cls(\n",
    "            top_level_keys=d.get('top_level_keys', {}),\n",
    "            scalar_values=d.get('scalar_values', {})\n",
    "        )\n",
    "    \n",
    "    def save(self, path: Path) -> None:\n",
    "        \"\"\"Save index to JSON file.\"\"\"\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(self.to_dict(), f, indent=2)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path: Path) -> 'JsonIndex':\n",
    "        \"\"\"Load index from JSON file.\"\"\"\n",
    "        with open(path, 'r') as f:\n",
    "            return cls.from_dict(json.load(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to check if character is whitespace\n",
    "def is_whitespace(ch: str) -> bool:\n",
    "    \"\"\"Check if character is JSON whitespace.\"\"\"\n",
    "    return ch in ' \\t\\n\\r'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected 'except' or 'finally' block (1391929143.py, line 138)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [15]\u001b[1;36m\u001b[0m\n\u001b[1;33m    else:\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m expected 'except' or 'finally' block\n"
     ]
    }
   ],
   "source": [
    "def build_index(\n",
    "    path_gz: Path,\n",
    "    json_index_file: Path,\n",
    "    gzip_index_file: Optional[Path] = None,\n",
    "    spacing: int = 300 * 1024,  # 300KB spacing for gzip index\n",
    "    chunk_size: int = 64 * 1024,  # 64KB chunks for overlapping\n",
    "    overlap_size: int = 1024  # 1KB overlap to handle values spanning chunks\n",
    ") -> JsonIndex:\n",
    "    \"\"\"\n",
    "    Build both gzip index and JSON structural index for an MRF file using pattern matching.\n",
    "    \n",
    "    Uses a two-pass approach:\n",
    "    1. Pattern matching pass: Find all \"key\": patterns, count occurrences, determine value types\n",
    "    2. Scalar extraction pass: Extract complete string scalar values for top-level keys\n",
    "    \n",
    "    For MRF files (top-level objects), this function:\n",
    "    - Identifies top-level keys by counting pattern occurrences (count=1 = top-level)\n",
    "    - Determines value types (scalar, array, object) immediately after colon\n",
    "    - Extracts complete scalar values (strings, numbers, booleans, null)\n",
    "    - Records start offsets for arrays and objects\n",
    "    \n",
    "    Args:\n",
    "        path_gz: Path to .json.gz file (must be a top-level object)\n",
    "        json_index_file: Path where to save the JSON structural index\n",
    "        gzip_index_file: Optional path where to save the gzip index (if None, uses default)\n",
    "        spacing: Spacing between seek points for gzip index (default: 300KB)\n",
    "        chunk_size: Size of chunks to read (default: 64KB)\n",
    "        overlap_size: Overlap between chunks to handle patterns spanning boundaries (default: 1KB)\n",
    "    \n",
    "    Returns:\n",
    "        JsonIndex with top-level keys (offsets for arrays/objects) and scalar_values (complete scalars)\n",
    "    \"\"\"\n",
    "    LOG.info(f\"Building indexes for {path_gz.name}...\")\n",
    "    \n",
    "    # Open file with indexed_gzip (builds gzip index during reading)\n",
    "    f = igzip.IndexedGzipFile(str(path_gz), spacing=spacing)\n",
    "    \n",
    "    try:\n",
    "        f.seek(0)\n",
    "        \n",
    "        # Pattern matching: track all key occurrences\n",
    "        key_occurrences = {}  # key_name -> count\n",
    "        key_info = {}  # key_name -> list of (chunk_idx, colon_offset, value_start_offset, value_type)\n",
    "        \n",
    "        # Overlap handling for pattern matching\n",
    "        overlap_buffer = b\"\"  # Keep as bytes for pattern matching\n",
    "        total_bytes_read = 0  # Track total bytes read from file\n",
    "        \n",
    "        LOG.info(f\"Pass 1: Pattern matching in chunks of {chunk_size:,} bytes with {overlap_size:,} byte overlap...\")\n",
    "        \n",
    "        chunk_count = 0\n",
    "        \n",
    "        # Pattern to match: \"key\":\n",
    "        # In bytes: b'\"' + key_name + b'\":'\n",
    "        # We'll scan for b'\":' and work backwards to find the key\n",
    "        \n",
    "        while True:\n",
    "            # Read chunk as bytes\n",
    "            chunk_bytes = f.read(chunk_size)\n",
    "            if not chunk_bytes:\n",
    "                break\n",
    "            \n",
    "            chunk_count += 1\n",
    "            \n",
    "            # Prepend overlap buffer for pattern continuity\n",
    "            if overlap_buffer:\n",
    "                chunk = overlap_buffer + chunk_bytes\n",
    "                chunk_start_offset = total_bytes_read - len(overlap_buffer)\n",
    "            else:\n",
    "                chunk = chunk_bytes\n",
    "                chunk_start_offset = total_bytes_read\n",
    "            \n",
    "            # Scan for \"key\": patterns\n",
    "            # Pattern: quote, key name, quote, colon\n",
    "            i = 0\n",
    "            while i < len(chunk) - 3:  # Need at least 4 bytes: \"key\":\n",
    "                # Look for quote-colon pattern: \":\n",
    "                if chunk[i:i+2] == b'\":':\n",
    "                    # Found potential end of key, work backwards to find start\n",
    "                    # Find the opening quote before this\n",
    "                    key_start = i\n",
    "                    while key_start > 0 and chunk[key_start] != ord('\"'):\n",
    "                        key_start -= 1\n",
    "                    \n",
    "                    if key_start < i and chunk[key_start] == ord('\"'):\n",
    "                        # Extract key name (between quotes)\n",
    "                        key_name_bytes = chunk[key_start+1:i]\n",
    "                        try:\n",
    "                            key_name = key_name_bytes.decode('utf-8')\n",
    "                            \n",
    "                            # Calculate byte offsets\n",
    "                            colon_offset = chunk_start_offset + i\n",
    "                            \n",
    "                            # Look ahead to determine value type (skip whitespace after colon)\n",
    "                            value_start = i + 2  # After '\":'\n",
    "                            while value_start < len(chunk) and chunk[value_start] in b' \\t\\n\\r':\n",
    "                                value_start += 1\n",
    "                            \n",
    "                            if value_start < len(chunk):\n",
    "                                value_char = chunk[value_start]\n",
    "                                value_start_offset = chunk_start_offset + value_start\n",
    "                                \n",
    "                                # Determine value type\n",
    "                                if value_char == ord('['):\n",
    "                                    value_type = 'array'\n",
    "                                elif value_char == ord('{'):\n",
    "                                    value_type = 'object'\n",
    "                                elif value_char == ord('\"'):\n",
    "                                    value_type = 'string'\n",
    "                                elif value_char in b'-0123456789':\n",
    "                                    value_type = 'number'\n",
    "                                elif value_char == ord('t'):\n",
    "                                    value_type = 'true'\n",
    "                                elif value_char == ord('f'):\n",
    "                                    value_type = 'false'\n",
    "                                elif value_char == ord('n'):\n",
    "                                    value_type = 'null'\n",
    "                                else:\n",
    "                                    value_type = 'unknown'\n",
    "                                \n",
    "                                # Track this key occurrence\n",
    "                                if key_name not in key_occurrences:\n",
    "                                    key_occurrences[key_name] = 0\n",
    "                                    key_info[key_name] = []\n",
    "                                \n",
    "                                key_occurrences[key_name] += 1\n",
    "                                key_info[key_name].append((\n",
    "                                    chunk_count,\n",
    "                                    colon_offset,\n",
    "                                    value_start_offset,\n",
    "                                    value_type\n",
    "                                ))\n",
    "                                \n",
    "                                LOG.debug(f\"Found key '{key_name}' (type: {value_type}) at offset {colon_offset:,}\")\n",
    "                            \n",
    "                            # Skip past this key to avoid matching it again\n",
    "                            i = value_start if value_start < len(chunk) else i + 2\n",
    "                        else:\n",
    "                            i += 1\n",
    "                    else:\n",
    "                        i += 1\n",
    "                else:\n",
    "                    i += 1\n",
    "            \n",
    "            # Update total bytes read\n",
    "            total_bytes_read += len(chunk_bytes)\n",
    "            \n",
    "            # Save overlap buffer\n",
    "            if len(chunk) > overlap_size:\n",
    "                overlap_buffer = chunk[-overlap_size:]\n",
    "            else:\n",
    "                overlap_buffer = chunk\n",
    "            \n",
    "            if chunk_count % 100 == 0:\n",
    "                LOG.debug(f\"Processed {chunk_count} chunks, found {len(key_occurrences)} unique keys...\")\n",
    "        \n",
    "        # Filter to top-level keys (occurrence count = 1)\n",
    "        top_level_keys = {}  # key -> offset (for arrays and objects)\n",
    "        scalar_values = {}  # key -> complete scalar value\n",
    "        top_level_string_keys = []  # Keys that need string extraction\n",
    "        \n",
    "        LOG.info(f\"Found {len(key_occurrences)} unique key(s), identifying top-level keys...\")\n",
    "        \n",
    "        for key_name, count in key_occurrences.items():\n",
    "            if count == 1:\n",
    "                # This is a top-level key\n",
    "                info = key_info[key_name][0]\n",
    "                colon_offset, value_start_offset, value_type = info[1], info[2], info[3]\n",
    "                \n",
    "                if value_type in ['array', 'object']:\n",
    "                    top_level_keys[key_name] = value_start_offset\n",
    "                    LOG.info(f\"Found top-level {value_type} key '{key_name}' at offset {value_start_offset:,}\")\n",
    "                elif value_type == 'string':\n",
    "                    top_level_string_keys.append((key_name, colon_offset, value_start_offset))\n",
    "                    LOG.debug(f\"Found top-level string key '{key_name}' at offset {value_start_offset:,}\")\n",
    "                elif value_type in ['number', 'true', 'false', 'null']:\n",
    "                    # These are short, we can extract them now\n",
    "                    # But we need to read from the file to get the complete value\n",
    "                    # For now, we'll do a second pass for all scalars\n",
    "                    top_level_string_keys.append((key_name, colon_offset, value_start_offset))\n",
    "        \n",
    "        LOG.info(f\"Identified {len(top_level_keys)} top-level array/object key(s)\")\n",
    "        LOG.info(f\"Identified {len(top_level_string_keys)} top-level scalar key(s) to extract\")\n",
    "        \n",
    "        # Pass 2: Extract scalar values (with overlap for strings)\n",
    "        if top_level_string_keys:\n",
    "            LOG.info(\"Pass 2: Extracting scalar values...\")\n",
    "            scalar_values = _extract_scalar_values(f, top_level_string_keys, chunk_size, overlap_size)\n",
    "        \n",
    "        # Create JSON index\n",
    "        json_index = JsonIndex(top_level_keys=top_level_keys, scalar_values=scalar_values)\n",
    "        \n",
    "        LOG.info(f\"JSON indexing complete:\")\n",
    "        LOG.info(f\"  - {len(json_index.scalar_values)} scalar value(s)\")\n",
    "        LOG.info(f\"  - {len(json_index.top_level_keys)} array/object key(s)\")\n",
    "        \n",
    "        if json_index.scalar_values:\n",
    "            LOG.info(f\"  Scalar keys: {list(json_index.scalar_values.keys())}\")\n",
    "        if json_index.top_level_keys:\n",
    "            LOG.info(f\"  Array/Object keys: {list(json_index.top_level_keys.keys())}\")\n",
    "        \n",
    "        # Save JSON index\n",
    "        json_index.save(json_index_file)\n",
    "        LOG.info(f\"JSON index saved to {json_index_file}\")\n",
    "        \n",
    "        # Export gzip index if requested\n",
    "        if gzip_index_file:\n",
    "            LOG.info(\"Exporting gzip index...\")\n",
    "            f.export_index(str(gzip_index_file))\n",
    "            LOG.info(f\"Gzip index saved to {gzip_index_file}\")\n",
    "        \n",
    "        return json_index\n",
    "    \n",
    "    finally:\n",
    "        f.close()\n",
    "\n",
    "\n",
    "def _extract_scalar_values(\n",
    "    f: igzip.IndexedGzipFile,\n",
    "    string_keys: list,\n",
    "    chunk_size: int,\n",
    "    overlap_size: int\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Extract complete scalar values for top-level string keys using overlapping chunks.\n",
    "    \n",
    "    Args:\n",
    "        f: IndexedGzipFile object (already opened)\n",
    "        string_keys: List of (key_name, colon_offset, value_start_offset) tuples\n",
    "        chunk_size: Size of chunks to read\n",
    "        overlap_size: Overlap between chunks\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping key names to their complete scalar values\n",
    "    \"\"\"\n",
    "    scalar_values = {}\n",
    "    \n",
    "    # For each key, extract its value\n",
    "    for key_name, colon_offset, value_start_offset in string_keys:\n",
    "        LOG.debug(f\"Extracting scalar value for '{key_name}' starting at offset {value_start_offset:,}\")\n",
    "        \n",
    "        f.seek(value_start_offset)\n",
    "        \n",
    "        # Read with overlap to get complete value\n",
    "        overlap_buffer = b\"\"\n",
    "        value_buffer = b\"\"\n",
    "        in_string = False\n",
    "        escape = False\n",
    "        \n",
    "        while True:\n",
    "            chunk_bytes = f.read(chunk_size)\n",
    "            if not chunk_bytes:\n",
    "                break\n",
    "            \n",
    "            if overlap_buffer:\n",
    "                chunk = overlap_buffer + chunk_bytes\n",
    "            else:\n",
    "                chunk = chunk_bytes\n",
    "            \n",
    "            # Extract string value\n",
    "            for i, byte_val in enumerate(chunk):\n",
    "                ch = chr(byte_val)\n",
    "                \n",
    "                if escape:\n",
    "                    escape = False\n",
    "                    value_buffer += bytes([byte_val])\n",
    "                elif ch == '\\\\':\n",
    "                    escape = True\n",
    "                    value_buffer += bytes([byte_val])\n",
    "                elif ch == '\"':\n",
    "                    if in_string:\n",
    "                        # End of string\n",
    "                        scalar_values[key_name] = value_buffer.decode('utf-8', errors='replace')\n",
    "                        LOG.debug(f\"Extracted string scalar '{key_name}': '{scalar_values[key_name][:50]}...'\")\n",
    "                        break\n",
    "                    else:\n",
    "                        # Start of string\n",
    "                        in_string = True\n",
    "                        value_buffer = b\"\"\n",
    "                else:\n",
    "                    if in_string:\n",
    "                        value_buffer += bytes([byte_val])\n",
    "            \n",
    "            if key_name in scalar_values:\n",
    "                break\n",
    "            \n",
    "            # Save overlap\n",
    "            if len(chunk) > overlap_size:\n",
    "                overlap_buffer = chunk[-overlap_size:]\n",
    "            else:\n",
    "                overlap_buffer = chunk\n",
    "        \n",
    "        # If we didn't find the closing quote, try to extract number/boolean/null\n",
    "        if key_name not in scalar_values:\n",
    "            f.seek(value_start_offset)\n",
    "            # Read a small chunk to get the value\n",
    "            value_chunk = f.read(100)  # Should be enough for numbers/booleans/null\n",
    "            if value_chunk:\n",
    "                try:\n",
    "                    value_str = value_chunk.decode('utf-8', errors='replace')\n",
    "                    # Extract until comma, }, or whitespace\n",
    "                    value = \"\"\n",
    "                    for ch in value_str:\n",
    "                        if ch in ',} \\t\\n\\r':\n",
    "                            break\n",
    "                        value += ch\n",
    "                    \n",
    "                    if value in ['true', 'false', 'null'] or (value and value[0] in '-0123456789'):\n",
    "                        scalar_values[key_name] = value\n",
    "                        LOG.debug(f\"Extracted scalar '{key_name}': {value}\")\n",
    "                except:\n",
    "                    pass\n",
    "    \n",
    "    return scalar_values\n",
    "            if not chunk_bytes:\n",
    "                break\n",
    "            \n",
    "            chunk_count += 1\n",
    "            \n",
    "            # Decode to string\n",
    "            try:\n",
    "                new_chunk = chunk_bytes.decode('utf-8')\n",
    "            except UnicodeDecodeError:\n",
    "                new_chunk = chunk_bytes.decode('utf-8', errors='replace')\n",
    "            \n",
    "            # Prepend overlap buffer to current chunk for continuity\n",
    "            if overlap_buffer:\n",
    "                chunk = overlap_buffer + new_chunk\n",
    "                chunk_start_offset = total_chars_read - len(overlap_buffer)\n",
    "            else:\n",
    "                chunk = new_chunk\n",
    "                chunk_start_offset = total_chars_read\n",
    "            \n",
    "            # Process ENTIRE chunk (including overlap) to maintain state and detect complete values\n",
    "            for i, ch in enumerate(chunk):\n",
    "                uncompressed_offset = chunk_start_offset + i\n",
    "                \n",
    "                # Skip whitespace before first character\n",
    "                if not first_char_processed:\n",
    "                    if is_whitespace(ch):\n",
    "                        continue\n",
    "                    if ch != '{':\n",
    "                        raise ValueError(f\"Expected top-level object ({{), but found: {ch}\")\n",
    "                    depth_object = 1\n",
    "                    first_char_processed = True\n",
    "                    continue\n",
    "                \n",
    "                # --- JSON STRUCTURE SCANNING ---\n",
    "                if in_string:\n",
    "                    if escape:\n",
    "                        escape = False\n",
    "                        if reading_key:\n",
    "                            current_key_buffer += ch\n",
    "                    elif ch == '\\\\':\n",
    "                        escape = True\n",
    "                        if reading_key:\n",
    "                            current_key_buffer += ch\n",
    "                    elif ch == '\"':\n",
    "                        in_string = False\n",
    "                        if reading_key:\n",
    "                            # Finished reading a key string\n",
    "                            # Only set pending_key if it's not empty\n",
    "                            if current_key_buffer:\n",
    "                                pending_key = current_key_buffer\n",
    "                            else:\n",
    "                                pending_key = None\n",
    "                            current_key_buffer = \"\"\n",
    "                            reading_key = False\n",
    "                else:\n",
    "                    # Check for scalar values after colon (non-whitespace characters)\n",
    "                    if (seen_colon_after_key and \n",
    "                        pending_key is not None and\n",
    "                        pending_key != \"\" and\n",
    "                        pending_key not in scalar_values and\n",
    "                        pending_key not in top_level_keys and\n",
    "                        depth_object == 1 and\n",
    "                        depth_array == 0 and\n",
    "                        not is_whitespace(ch) and\n",
    "                        value_start_offset is None):\n",
    "                        # This is the start of a value - determine type\n",
    "                        value_start_offset = uncompressed_offset\n",
    "                        \n",
    "                        if ch == '\"':\n",
    "                            # String scalar - will be extracted when we see closing quote\n",
    "                            in_string = True\n",
    "                            current_scalar_buffer = \"\"\n",
    "                        elif ch in '-0123456789':\n",
    "                            # Number scalar - extract until comma, }, or whitespace\n",
    "                            current_scalar_buffer = ch\n",
    "                        elif ch == 't':\n",
    "                            # Could be 'true'\n",
    "                            current_scalar_buffer = ch\n",
    "                        elif ch == 'f':\n",
    "                            # Could be 'false'\n",
    "                            current_scalar_buffer = ch\n",
    "                        elif ch == 'n':\n",
    "                            # Could be 'null'\n",
    "                            current_scalar_buffer = ch\n",
    "                        else:\n",
    "                            # Not a scalar, reset\n",
    "                            value_start_offset = None\n",
    "                    \n",
    "                    # Continue extracting scalar value if we're in the middle of one\n",
    "                    elif value_start_offset is not None and pending_key is not None:\n",
    "                        if in_string:\n",
    "                            # String extraction - handle escape sequences\n",
    "                            if escape:\n",
    "                                escape = False\n",
    "                                current_scalar_buffer += ch\n",
    "                            elif ch == '\\\\':\n",
    "                                escape = True\n",
    "                                current_scalar_buffer += ch\n",
    "                            elif ch == '\"':\n",
    "                                # End of string scalar\n",
    "                                in_string = False\n",
    "                                scalar_values[pending_key] = current_scalar_buffer\n",
    "                                LOG.debug(f\"Found string scalar '{pending_key}': '{current_scalar_buffer[:50]}...'\")\n",
    "                                pending_key = None\n",
    "                                seen_colon_after_key = False\n",
    "                                value_start_offset = None\n",
    "                                current_scalar_buffer = \"\"\n",
    "                            else:\n",
    "                                current_scalar_buffer += ch\n",
    "                        elif current_scalar_buffer:\n",
    "                            # Number, true, false, or null extraction\n",
    "                            if ch in '-0123456789.eE+':\n",
    "                                # Continue number\n",
    "                                current_scalar_buffer += ch\n",
    "                            elif current_scalar_buffer == 't' and ch == 'r':\n",
    "                                current_scalar_buffer += ch\n",
    "                            elif current_scalar_buffer == 'tr' and ch == 'u':\n",
    "                                current_scalar_buffer += ch\n",
    "                            elif current_scalar_buffer == 'tru' and ch == 'e':\n",
    "                                # Complete 'true'\n",
    "                                scalar_values[pending_key] = 'true'\n",
    "                                LOG.debug(f\"Found boolean scalar '{pending_key}': true\")\n",
    "                                pending_key = None\n",
    "                                seen_colon_after_key = False\n",
    "                                value_start_offset = None\n",
    "                                current_scalar_buffer = \"\"\n",
    "                            elif current_scalar_buffer == 'f' and ch == 'a':\n",
    "                                current_scalar_buffer += ch\n",
    "                            elif current_scalar_buffer == 'fa' and ch == 'l':\n",
    "                                current_scalar_buffer += ch\n",
    "                            elif current_scalar_buffer == 'fal' and ch == 's':\n",
    "                                current_scalar_buffer += ch\n",
    "                            elif current_scalar_buffer == 'fals' and ch == 'e':\n",
    "                                # Complete 'false'\n",
    "                                scalar_values[pending_key] = 'false'\n",
    "                                LOG.debug(f\"Found boolean scalar '{pending_key}': false\")\n",
    "                                pending_key = None\n",
    "                                seen_colon_after_key = False\n",
    "                                value_start_offset = None\n",
    "                                current_scalar_buffer = \"\"\n",
    "                            elif current_scalar_buffer == 'n' and ch == 'u':\n",
    "                                current_scalar_buffer += ch\n",
    "                            elif current_scalar_buffer == 'nu' and ch == 'l':\n",
    "                                current_scalar_buffer += ch\n",
    "                            elif current_scalar_buffer == 'nul' and ch == 'l':\n",
    "                                # Complete 'null'\n",
    "                                scalar_values[pending_key] = 'null'\n",
    "                                LOG.debug(f\"Found null scalar '{pending_key}'\")\n",
    "                                pending_key = None\n",
    "                                seen_colon_after_key = False\n",
    "                                value_start_offset = None\n",
    "                                current_scalar_buffer = \"\"\n",
    "                            elif ch in ',}':\n",
    "                                # End of number (or invalid token)\n",
    "                                if current_scalar_buffer and current_scalar_buffer[0] in '-0123456789':\n",
    "                                    # It's a number\n",
    "                                    scalar_values[pending_key] = current_scalar_buffer\n",
    "                                    LOG.debug(f\"Found number scalar '{pending_key}': {current_scalar_buffer}\")\n",
    "                                    pending_key = None\n",
    "                                    seen_colon_after_key = False\n",
    "                                    value_start_offset = None\n",
    "                                    current_scalar_buffer = \"\"\n",
    "                                else:\n",
    "                                    # Invalid, reset\n",
    "                                    value_start_offset = None\n",
    "                                    current_scalar_buffer = \"\"\n",
    "                            elif is_whitespace(ch):\n",
    "                                # Whitespace might end a number\n",
    "                                if current_scalar_buffer and current_scalar_buffer[0] in '-0123456789':\n",
    "                                    scalar_values[pending_key] = current_scalar_buffer\n",
    "                                    LOG.debug(f\"Found number scalar '{pending_key}': {current_scalar_buffer}\")\n",
    "                                    pending_key = None\n",
    "                                    seen_colon_after_key = False\n",
    "                                    value_start_offset = None\n",
    "                                    current_scalar_buffer = \"\"\n",
    "                            else:\n",
    "                                # Invalid character, reset\n",
    "                                value_start_offset = None\n",
    "                                current_scalar_buffer = \"\"\n",
    "                    \n",
    "                    if ch == '\"':\n",
    "                        if not in_string:\n",
    "                            in_string = True\n",
    "                            if depth_object == 1 and depth_array == 0 and not seen_colon_after_key:\n",
    "                                # Starting to read a key\n",
    "                                reading_key = True\n",
    "                                current_key_buffer = \"\"\n",
    "                                pending_key = None\n",
    "                    \n",
    "                    elif ch == '{':\n",
    "                        # Check if this is a top-level object value BEFORE incrementing depth\n",
    "                        if (depth_object == 1 and \n",
    "                            depth_array == 0 and\n",
    "                            seen_colon_after_key and \n",
    "                            pending_key is not None and\n",
    "                            pending_key != \"\" and  # Ensure key is not empty\n",
    "                            pending_key not in top_level_keys and\n",
    "                            pending_key not in scalar_values):\n",
    "                            # This is a top-level object value\n",
    "                            top_level_keys[pending_key] = uncompressed_offset\n",
    "                            LOG.info(f\"Found object key '{pending_key}' at offset {uncompressed_offset}\")\n",
    "                            pending_key = None\n",
    "                            seen_colon_after_key = False\n",
    "                            value_start_offset = None\n",
    "                        \n",
    "                        depth_object += 1\n",
    "                    \n",
    "                    elif ch == '}':\n",
    "                        # Check if we have a pending scalar that ended with }\n",
    "                        if value_start_offset is not None and pending_key is not None and current_scalar_buffer:\n",
    "                            if current_scalar_buffer[0] in '-0123456789':\n",
    "                                scalar_values[pending_key] = current_scalar_buffer\n",
    "                                LOG.debug(f\"Found number scalar '{pending_key}': {current_scalar_buffer}\")\n",
    "                                pending_key = None\n",
    "                                seen_colon_after_key = False\n",
    "                                value_start_offset = None\n",
    "                                current_scalar_buffer = \"\"\n",
    "                        \n",
    "                        depth_object -= 1\n",
    "                        if depth_object == 0:\n",
    "                            # End of top-level object\n",
    "                            break\n",
    "                        # Reset state when closing nested object\n",
    "                        value_start_offset = None\n",
    "                        current_scalar_buffer = \"\"\n",
    "                    \n",
    "                    elif ch == '[':\n",
    "                        # Check if this is a top-level array value BEFORE incrementing depth\n",
    "                        if (depth_object == 1 and \n",
    "                            depth_array == 0 and\n",
    "                            seen_colon_after_key and \n",
    "                            pending_key is not None and\n",
    "                            pending_key != \"\" and  # Ensure key is not empty\n",
    "                            pending_key not in top_level_keys and\n",
    "                            pending_key not in scalar_values):\n",
    "                            # This is a top-level array value\n",
    "                            top_level_keys[pending_key] = uncompressed_offset\n",
    "                            LOG.info(f\"Found array key '{pending_key}' at offset {uncompressed_offset}\")\n",
    "                            pending_key = None\n",
    "                            seen_colon_after_key = False\n",
    "                            value_start_offset = None\n",
    "                        \n",
    "                        depth_array += 1\n",
    "                        value_start_offset = None\n",
    "                        current_scalar_buffer = \"\"\n",
    "                    \n",
    "                    elif ch == ']':\n",
    "                        depth_array -= 1\n",
    "                        value_start_offset = None\n",
    "                        current_scalar_buffer = \"\"\n",
    "                    \n",
    "                    elif ch == ':':\n",
    "                        # Colon means the previous string was a key\n",
    "                        if (pending_key is not None and \n",
    "                            pending_key != \"\" and  # Ensure key is not empty\n",
    "                            depth_object == 1 and \n",
    "                            depth_array == 0):\n",
    "                            seen_colon_after_key = True\n",
    "                            value_start_offset = None  # Reset, will be set when we see value start\n",
    "                            current_scalar_buffer = \"\"\n",
    "                            LOG.debug(f\"Saw colon after key '{pending_key}' at offset {uncompressed_offset}\")\n",
    "                        else:\n",
    "                            # If pending_key is empty or None, reset it\n",
    "                            pending_key = None\n",
    "                    \n",
    "                    elif ch == ',':\n",
    "                        # Check if we have a pending scalar that ended with comma\n",
    "                        if value_start_offset is not None and pending_key is not None and current_scalar_buffer:\n",
    "                            if current_scalar_buffer[0] in '-0123456789':\n",
    "                                scalar_values[pending_key] = current_scalar_buffer\n",
    "                                LOG.debug(f\"Found number scalar '{pending_key}': {current_scalar_buffer}\")\n",
    "                                pending_key = None\n",
    "                                seen_colon_after_key = False\n",
    "                                value_start_offset = None\n",
    "                                current_scalar_buffer = \"\"\n",
    "                        \n",
    "                        # Comma means we're moving to next key-value pair\n",
    "                        if pending_key is not None and not seen_colon_after_key:\n",
    "                            # Key wasn't followed by colon, so it was a value\n",
    "                            pending_key = None\n",
    "                        seen_colon_after_key = False\n",
    "                        value_start_offset = None\n",
    "                        current_scalar_buffer = \"\"\n",
    "            \n",
    "            # Update total characters read (only count new chunk, not overlap)\n",
    "            total_chars_read += len(new_chunk)\n",
    "            \n",
    "            # Save last part of chunk as overlap buffer (for next iteration)\n",
    "            if len(chunk) > overlap_size:\n",
    "                overlap_buffer = chunk[-overlap_size:]\n",
    "            else:\n",
    "                overlap_buffer = chunk\n",
    "            \n",
    "            if chunk_count % 100 == 0:\n",
    "                LOG.debug(f\"Processed {chunk_count} chunks, found {len(scalar_values)} scalars, {len(top_level_keys)} arrays/objects...\")\n",
    "        \n",
    "            # Create JSON index\n",
    "            json_index = JsonIndex(top_level_keys=top_level_keys, scalar_values=scalar_values)\n",
    "\n",
    "            LOG.info(f\"JSON indexing complete:\")\n",
    "            LOG.info(f\"  - {len(json_index.scalar_values)} scalar value(s)\")\n",
    "            LOG.info(f\"  - {len(json_index.top_level_keys)} array/object key(s)\")\n",
    "\n",
    "            if json_index.scalar_values:\n",
    "                LOG.info(f\"  Scalar keys: {list(json_index.scalar_values.keys())}\")\n",
    "            if json_index.top_level_keys:\n",
    "                LOG.info(f\"  Array/Object keys: {list(json_index.top_level_keys.keys())}\")\n",
    "\n",
    "            # Save JSON index\n",
    "            json_index.save(json_index_file)\n",
    "            LOG.info(f\"JSON index saved to {json_index_file}\")\n",
    "\n",
    "            # Export gzip index if requested\n",
    "            if gzip_index_file:\n",
    "                LOG.info(\"Exporting gzip index...\")\n",
    "                f.export_index(str(gzip_index_file))\n",
    "                LOG.info(f\"Gzip index saved to {gzip_index_file}\")\n",
    "\n",
    "            return json_index\n",
    "    \n",
    "    finally:\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convenience Wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `build_index` function uses overlapping chunks to ensure complete scalar values are captured even if they span chunk boundaries. It identifies three types of top-level values:\n",
    "\n",
    "1. **Scalars**: Complete string values extracted and stored in `scalar_values`\n",
    "2. **Arrays**: Start offset recorded in `top_level_keys` (value starts with `[`)\n",
    "3. **Objects**: Start offset recorded in `top_level_keys` (value starts with `{` at top level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convenience wrapper function\n",
    "def build_index_wrapper(\n",
    "    input_file: Path,\n",
    "    json_index_file: Path,\n",
    "    gzip_index_file: Optional[Path] = None,\n",
    "    spacing: int = 300 * 1024  # 300KB default (minimum recommended for indexed_gzip is 64KB)\n",
    ") -> JsonIndex:\n",
    "    \"\"\"\n",
    "    Convenience wrapper to build both indexes.\n",
    "    \n",
    "    Args:\n",
    "        input_file: Path to .json.gz file\n",
    "        json_index_file: Path where to save JSON structural index\n",
    "        gzip_index_file: Optional path where to save gzip index\n",
    "        spacing: Spacing for gzip index (default: 300KB, minimum recommended: 64KB)\n",
    "    \n",
    "    Returns:\n",
    "        JsonIndex object\n",
    "    \"\"\"\n",
    "    if not input_file.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {input_file}\")\n",
    "    \n",
    "    # Ensure spacing is at least 64KB (minimum for indexed_gzip)\n",
    "    if spacing < 64 * 1024:\n",
    "        LOG.warning(f\"Spacing {spacing} bytes is too small, using minimum 64KB\")\n",
    "        spacing = 64 * 1024\n",
    "    \n",
    "    json_index = build_index(input_file, json_index_file, gzip_index_file, spacing=spacing)\n",
    "    \n",
    "    print(f\"\\nIndex summary:\")\n",
    "    print(f\"  JSON index: {json_index_file}\")\n",
    "    if gzip_index_file:\n",
    "        print(f\"  Gzip index: {gzip_index_file}\")\n",
    "    print(f\"  Scalar values: {len(json_index.scalar_values)}\")\n",
    "    if json_index.scalar_values:\n",
    "        print(f\"  Scalar keys: {list(json_index.scalar_values.keys())}\")\n",
    "    print(f\"  Array/Object keys: {len(json_index.top_level_keys)}\")\n",
    "    if json_index.top_level_keys:\n",
    "        print(f\"  Array/Object keys: {list(json_index.top_level_keys.keys())}\")\n",
    "    \n",
    "    return json_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrf_file = Path(\"D://2026-01_890_58B0_in-network-rates_58_of_60.json.gz\")\n",
    "json_index_file = Path(\"D://2026-01_890_58B0_in-network-rates_58_of_60.json.gz.index\")\n",
    "gzip_index_file = Path(\"D://2026-01_890_58B0_in-network-rates_58_of_60.json.gz.gzidx\")\n",
    "\n",
    "# Step 1: Build both indexes (read-only, creates index files)\n",
    "# Note: spacing defaults to 300KB, minimum is 64KB for indexed_gzip\n",
    "index = build_index_wrapper(mrf_file, json_index_file, gzip_index_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production Notes\n",
    "\n",
    "### Limitations of Current Implementation\n",
    "\n",
    "1. **Gzip State Serialization**: The current implementation uses a simplified approach for gzip state. For production use, you would need to:\n",
    "   - Use `zlib.decompressobj()` directly instead of `gzip.GzipFile()`\n",
    "   - Serialize the actual zlib decompressor state (which is complex)\n",
    "   - Or use a library that supports gzip seeking (like `indexed_gzip`)\n",
    "\n",
    "2. **Checkpoint Accuracy**: The current checkpoints are approximate. For exact seeking, you'd need to:\n",
    "   - Store the exact zlib window state\n",
    "   - Store the bit buffer state\n",
    "   - Handle gzip headers and footers correctly\n",
    "\n",
    "3. **Memory Usage**: For very large files, consider:\n",
    "   - Streaming checkpoint creation\n",
    "   - Incremental index updates\n",
    "   - Using memory-mapped files\n",
    "\n",
    "### Alternative Approaches\n",
    "\n",
    "For production, consider:\n",
    "- Using `indexed_gzip` library (https://github.com/pauldmccarthy/indexed_gzip)\n",
    "- Using `zstandadr` compression with seekable format\n",
    "- Pre-processing files into chunked formats (e.g., line-delimited JSON per chunk)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
